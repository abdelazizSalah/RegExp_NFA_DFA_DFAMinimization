{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexer\n",
    "* This Component is responsible for breaking the input into tokens.\n",
    "* And it escape all spaces and newlines.\n",
    "* and we implemented it mainly for 2 reasons:\n",
    "    1. We will need to do parsing, and parsing on raw characters is ugly\n",
    "\n",
    "    2. (Optional) We will need to handle escapes :\n",
    "     Suppose you want to match the literal string '(', how\n",
    "     would you do it ?\n",
    "     If your regex engine doesn't handle escapes, you\n",
    "     can't, but with escapes it's simply the regex \"\\(\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First of all we need to define all the tokens' types\n",
    "DEBUG = True\n",
    "from enum import Enum\n",
    "class TokenTypes(Enum):\n",
    "    OR = 1 \n",
    "    Astrisk = 2 \n",
    "    Plus = 3 \n",
    "    QuestionMark = 4\n",
    "    OpenSquareBracket = 5 \n",
    "    ClosedSquareBracket = 6\n",
    "    OpenBracket = 7\n",
    "    ClosedBracket = 8\n",
    "    Hiphen = 9 \n",
    "    Dot = 10\n",
    "    LiteralChar = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we need to define a class for the tokens \n",
    "class Token: \n",
    "    def __init__(self, tokenType, value):\n",
    "        self.tokenType = tokenType\n",
    "        self.value = value # the string value of the token, and is used in case of LiteralChar.\n",
    "\n",
    "# we need to define a class for token stream, and a pointer, which is initialized to 0, because they will be used later by the \n",
    "class TokenStream: \n",
    "    def __init__ (self, tokenStream = [],tokenPointer=0): \n",
    "        self.tokenStream = tokenStream\n",
    "        self.tokenPointer = tokenPointer\n",
    "    def resetStream (self): \n",
    "        self.tokenStream = []\n",
    "        self.tokenPointer = 0\n",
    "    def advanceTokenPointer(self):\n",
    "        self.tokenPointer += 1\n",
    "    def getCurrentToken(self):\n",
    "        return self.tokenStream[self.tokenPointer]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets implement the Lexer Class \n",
    "class Lexer: \n",
    "    @staticmethod\n",
    "    def lexReg(regex:str): \n",
    "        '''\n",
    "            This function is responsible for lexing the regular expression and returning a list of tokens.\n",
    "            Input: \n",
    "                regex: a string representing the regular expression.\n",
    "            Output:\n",
    "                a list of tokens.\n",
    "        '''\n",
    "        # create a map which maps each character to its corresponding token type.\n",
    "        metaCharactersMap = { \n",
    "            '|': TokenTypes.OR,\n",
    "            '*': TokenTypes.Astrisk,\n",
    "            '+': TokenTypes.Plus,\n",
    "            '?': TokenTypes.QuestionMark,\n",
    "            '[': TokenTypes.OpenSquareBracket,\n",
    "            ']': TokenTypes.ClosedSquareBracket,\n",
    "            '(': TokenTypes.OpenBracket,\n",
    "            ')': TokenTypes.ClosedBracket,\n",
    "            '-': TokenTypes.Hiphen,\n",
    "            '.': TokenTypes.Dot\n",
    "            # LiteralCharacters are any other characters that are not in the map.\n",
    "        }\n",
    "\n",
    "        # define spaceChar and escapeChar\n",
    "        spaceChar = ' '\n",
    "        escapeChar = '\\\\'\n",
    "\n",
    "        # initialize an empty token stream\n",
    "        tokens = TokenStream()\n",
    "        tokens.resetStream()\n",
    "\n",
    "        # we will need to use two pointers approach, one holds the previous character, and the other holds the current character.\n",
    "        previousChar = None\n",
    "        for char in regex : \n",
    "            if char == spaceChar: \n",
    "                continue\n",
    "            # skip all escapes\n",
    "            if char == escapeChar: \n",
    "                # if the previous character is escape character, then we need to add the current character as a LiteralChar token.\n",
    "                if previousChar == escapeChar:\n",
    "                    tokens.tokenStream.append(Token(TokenTypes.LiteralChar, char))\n",
    "                    previousChar = None\n",
    "                    continue\n",
    "                # else, then we are preparing ourself to escape the next character.\n",
    "                previousChar = char\n",
    "                continue\n",
    "            # check if the current character meta character and not preceeded with space \n",
    "            if char in metaCharactersMap and previousChar != escapeChar: \n",
    "                # add the current character to the token stream\n",
    "                tokens.tokenStream.append(Token(metaCharactersMap[char], char))\n",
    "            else: \n",
    "                # in this case we are trying to match the exact symbol, so we need to add it as a LiteralChar token.\n",
    "                tokens.tokenStream.append(Token(TokenTypes.LiteralChar, char))\n",
    "            previousChar = char\n",
    "        return tokens\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TokenTypes.LiteralChar a\n",
      "TokenTypes.LiteralChar b\n",
      "TokenTypes.Astrisk *\n",
      "TokenTypes.LiteralChar c\n",
      "TokenTypes.Plus +\n",
      "TokenTypes.LiteralChar d\n",
      "TokenTypes.LiteralChar e\n",
      "TokenTypes.QuestionMark ?\n",
      "TokenTypes.OpenBracket (\n",
      "TokenTypes.LiteralChar f\n",
      "TokenTypes.OR |\n",
      "TokenTypes.LiteralChar g\n",
      "TokenTypes.OR |\n",
      "TokenTypes.LiteralChar h\n",
      "TokenTypes.ClosedBracket )\n",
      "TokenTypes.OR |\n",
      "TokenTypes.LiteralChar m\n",
      "TokenTypes.LiteralChar r\n",
      "TokenTypes.OR |\n",
      "TokenTypes.LiteralChar n\n",
      "TokenTypes.OR |\n",
      "TokenTypes.OpenSquareBracket [\n",
      "TokenTypes.LiteralChar p\n",
      "TokenTypes.LiteralChar q\n",
      "TokenTypes.ClosedSquareBracket ]\n"
     ]
    }
   ],
   "source": [
    "# Lets Test the Lexer \n",
    "def testLexer(): \n",
    "    # regex = 'a?b(cd|ef)[a-z]'\n",
    "    \n",
    "    regex = 'ab*c+de?(f|g|h)|mr|n|[pq]' #\\\\\n",
    "    tokens = Lexer.lexReg(regex)\n",
    "    for token in tokens.tokenStream: \n",
    "        print(token.tokenType, token.value)\n",
    "\n",
    "testLexer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regex_Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to define our AST Nodes \n",
    "class AstNode: \n",
    "    pass # this is an abstract class \n",
    "\n",
    "'''\n",
    "    What are our cases? \n",
    "        1. + -> One or more node class\n",
    "        2. * -> Zero or more node class\n",
    "        3. ? -> Zero or one class\n",
    "        4. | -> OR class\n",
    "        5. [] -> set of characters class\n",
    "        6. abcd -> Sequence of characters class\n",
    "        7. LiteralChar -> Literal Character class\n",
    "'''\n",
    "class PlusNode(AstNode):\n",
    "    left: AstNode\n",
    "    def __init__(self, left):\n",
    "        self.left = left\n",
    "\n",
    "class AstriskNode(AstNode):\n",
    "    left: AstNode\n",
    "    def __init__(self, left):\n",
    "        self.left = left\n",
    "\n",
    "class QuestionMarkNode(AstNode):\n",
    "    left: AstNode\n",
    "    def __init__(self, left):\n",
    "        self.left = left\n",
    "    \n",
    "class OrNode(AstNode):\n",
    "    left: AstNode\n",
    "    right: AstNode\n",
    "    def __init__(self, left, right):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "class SetOfCharactersNode(AstNode):\n",
    "    characters: set  # maybe strings or pairs as in case of [0-9]\n",
    "    def __init__(self, characters):\n",
    "        self.characters = characters    \n",
    "\n",
    "class SequenceOfCharactersNode(AstNode):\n",
    "    left: AstNode\n",
    "    right: AstNode\n",
    "    def __init__(self, left, right):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "class LiteralCharNode(AstNode):\n",
    "    value: str\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now lets define our grammar \n",
    "\n",
    "1. parse -> parseReg\n",
    "2. parseReg -> parseOr\n",
    "2. parseOr -> parseSeq (| parseSeq)*\n",
    "2. parseSeq -> parseQuantified (parseQuantified)*\n",
    "2. parseQuantified -> parseBase (+ | * | ?)?\n",
    "2. parseBase -> LiteralChar | SetOfCharacters | ( parseReg )\n",
    "2. so we just need to implement one function for each of those following the recursive descent parsing approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. parseBase -> because it is our basecase, so we must implement the functions bottom up.\n",
    "def parseSquareBrackets(tokenStream, tokenIdx):\n",
    "    '''\n",
    "        Our content may have different patterns: \n",
    "            1. a-z -> one range\n",
    "            2. abc0-9 -> certain Literals and a range\n",
    "            3. xyz -> certain Literals\n",
    "            4. A-Z0-9 multiple Ranges \n",
    "            5. all above.\n",
    "        how to handle them? \n",
    "        1. any range should be treated as a tuple of 2 characters\n",
    "            first is the starting index\n",
    "            second is the ending index. \n",
    "        2. any sequence of characters \n",
    "    '''\n",
    "    characters = [] \n",
    "    prevIsDash = False\n",
    "    # We Should iterate till we find a ] <= \n",
    "    while tokenStream[tokenIdx].tokenType != TokenTypes.ClosedSquareBracket:\n",
    "        if tokenStream[tokenIdx].tokenType == TokenTypes.Hiphen:\n",
    "            prevIsDash = True\n",
    "        elif prevIsDash:\n",
    "            # get the last appended char as the starting character\n",
    "            startingChar = characters.pop()\n",
    "            # get the current character as an ending character\n",
    "            endingChar = tokenStream[tokenIdx].value\n",
    "            # push in the characters a tuple\n",
    "            characters.append((startingChar, endingChar))\n",
    "            # set the prevIsDash back to False \n",
    "            prevIsDash = False\n",
    "        elif tokenStream[tokenIdx].tokenType == TokenTypes.LiteralChar:\n",
    "            characters.append(tokenStream[tokenIdx].value)\n",
    "        tokenIdx += 1\n",
    "    \n",
    "    # here we should assign it to set, but I left it as List.\n",
    "    return SetOfCharactersNode(characters), tokenIdx\n",
    "\n",
    "def parseBase(tokenStream, tokenIdx):\n",
    "    '''\n",
    "        This function is responsible for parsing the base cases, or applying a recursive call on bracktes\n",
    "        Implmenting this grammar:  parseBase -> LiteralChar | SetOfCharacters | ( parseReg )\n",
    "\n",
    "        Input: \n",
    "            tokenStream: a list of tokens.\n",
    "            tokenIdx: the current index of the token.\n",
    "        Output:\n",
    "            an AST node, and the new index\n",
    "    '''\n",
    "\n",
    "    # we have three cases, LiteralChar, SetOfCharacters, and ( parseReg )\n",
    "\n",
    "    # Extracting the token \n",
    "    token:Token = tokenStream[tokenIdx]\n",
    "    tokenIdx += 1 \n",
    "\n",
    "    # LiteralChar\n",
    "    if token.tokenType == TokenTypes.LiteralChar: \n",
    "        return LiteralCharNode(token.value), tokenIdx\n",
    "    \n",
    "    # set of Characters. \n",
    "    if token.tokenType == TokenTypes.OpenSquareBracket: \n",
    "        # we need a utility function to parse the data inside the square brackets.\n",
    "        return parseSquareBrackets(tokenStream, tokenIdx)\n",
    "\n",
    "    if token.tokenType == TokenTypes.OpenBracket: \n",
    "        # we need to parse the regular expression inside the brackets.\n",
    "        parsedReg, tokenIdx =  parseRegex(tokenStream, tokenIdx)\n",
    "        return parsedReg, tokenIdx+1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseQuantified(tokensStream, tokenIdx): \n",
    "    '''\n",
    "        This function is responsible for parsing the quantified cases.\n",
    "        it implements this grammar: \n",
    "            parseQuantified -> parseBase (+ | * | ?)?\n",
    "    '''\n",
    "\n",
    "    # first we assume that we have only one operand \n",
    "    leftOperand, tokenIdx = parseBase(tokensStream, tokenIdx)\n",
    "\n",
    "    # now we need to check if there are more operands \n",
    "    if tokenIdx >= len(tokensStream):\n",
    "        # no more operands, just return the leftOperand \n",
    "        return leftOperand, tokenIdx\n",
    "    \n",
    "    # now we have to check on the operator (*,+,?)\n",
    "    token = tokensStream[tokenIdx]\n",
    "    if token.tokenType == TokenTypes.Astrisk:\n",
    "        return AstriskNode(leftOperand), tokenIdx + 1\n",
    "    if token.tokenType == TokenTypes.Plus:\n",
    "        return PlusNode(leftOperand), tokenIdx + 1\n",
    "    if token.tokenType == TokenTypes.QuestionMark:\n",
    "        return QuestionMarkNode(leftOperand), tokenIdx + 1\n",
    "    \n",
    "    # we should never reach here, however, return the leftoperand for the function definition\n",
    "    return leftOperand, tokenIdx+1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseSequence(tokensStream, tokenIdx): \n",
    "    '''\n",
    "        This function is responsible for parsing the sequence of characters.\n",
    "        it implements this grammar: \n",
    "            parseSequence -> parseQuantified (parseQuantified)*\n",
    "    '''\n",
    "    # first we assume that we have only one operand\n",
    "    leftOperand = parseQuantified(tokensStream, tokenIdx)\n",
    "\n",
    "    # now we need to check if we have more elements \n",
    "    if tokenIdx >= len(tokensStream):\n",
    "        # no more elements \n",
    "        return leftOperand, tokenIdx\n",
    "    \n",
    "    # we need to continue parsing all sequence like abcd, or if we have operator we should also stop. \n",
    "    # to do so, we have to have a stoping condition\n",
    "    # we will stop if we have a closed bracket, or an OR operator.\n",
    "    while tokenIdx < len(tokensStream) and tokensStream[tokenIdx].tokenType not in [TokenTypes.OR, TokenTypes.ClosedBracket]:\n",
    "        rightOperand, tokenIdx = parseQuantified(tokensStream, tokenIdx) # token index is incremented here implecitly.\n",
    "        leftOperand = SequenceOfCharactersNode(leftOperand, rightOperand) # recursive assignment, cascading the elemets in the leftOperand.\n",
    "    return leftOperand, tokenIdx \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseOr (tokensStream, tokenIdx): \n",
    "    '''\n",
    "        This function is responsible for parsing the OR operator.\n",
    "        it implements this grammar: \n",
    "            parseOr -> parseSequence (| parseSequence)*\n",
    "    '''\n",
    "    # first we assume that we have only one operand\n",
    "    leftOperand = parseSequence(tokensStream, tokenIdx)\n",
    "\n",
    "    # now we need to check if we have more elements\n",
    "    if tokenIdx >= len(tokensStream): \n",
    "        return leftOperand, tokenIdx\n",
    "    \n",
    "    # now we still have elements, we just need to do recursion, as we see or operator \n",
    "    while tokenIdx < len(tokensStream) and tokensStream[tokenIdx].tokenType == TokenTypes.OR:\n",
    "        rightOperand, tokenIdx = parseSequence(tokensStream, tokenIdx+1)\n",
    "        leftOperand = OrNode(leftOperand, rightOperand)\n",
    "    \n",
    "    return leftOperand, tokenIdx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseRegex (tokensStream, index):\n",
    "    '''\n",
    "        This function should implement the grammar:\n",
    "            parseReg -> parseOr\n",
    "    ''' \n",
    "    return parseOr(tokensStream, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(tokensStream): \n",
    "    expression, _ = parseRegex(tokensStream, 0)\n",
    "    return expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [98]\u001b[0m, in \u001b[0;36m<cell line: 37>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Its time to test our logic. \u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# regex = 'ab*c+de?(f|g|h)|mr|n|[pq]'\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# regex = 'a+'\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# tokens = Lexer.lexReg(regex)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# ast = parse(tokens.tokenStream)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# print(ast)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m toks \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      8\u001b[0m     Token(TokenTypes\u001b[38;5;241m.\u001b[39mLiteralChar,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m      9\u001b[0m     Token(TokenTypes\u001b[38;5;241m.\u001b[39mLiteralChar,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     35\u001b[0m     Token(TokenTypes\u001b[38;5;241m.\u001b[39mClosedBracket,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     36\u001b[0m ]\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoks\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Input \u001b[1;32mIn [79]\u001b[0m, in \u001b[0;36mparse\u001b[1;34m(tokensStream)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(tokensStream): \n\u001b[1;32m----> 2\u001b[0m     expression, _ \u001b[38;5;241m=\u001b[39m \u001b[43mparseRegex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokensStream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m expression\n",
      "Input \u001b[1;32mIn [78]\u001b[0m, in \u001b[0;36mparseRegex\u001b[1;34m(tokensStream, index)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparseRegex\u001b[39m (tokensStream, index):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m        This function should implement the grammar:\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m            parseReg -> parseOr\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m \n\u001b[1;32m----> 6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparseOr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokensStream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [77]\u001b[0m, in \u001b[0;36mparseOr\u001b[1;34m(tokensStream, tokenIdx)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m    This function is responsible for parsing the OR operator.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m    it implements this grammar: \u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m        parseOr -> parseSequence (| parseSequence)*\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# first we assume that we have only one operand\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m leftOperand \u001b[38;5;241m=\u001b[39m \u001b[43mparseSequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokensStream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenIdx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# now we need to check if we have more elements\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenIdx \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(tokensStream): \n",
      "Input \u001b[1;32mIn [76]\u001b[0m, in \u001b[0;36mparseSequence\u001b[1;34m(tokensStream, tokenIdx)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# we need to continue parsing all sequence like abcd, or if we have operator we should also stop. \u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# to do so, we have to have a stoping condition\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# we will stop if we have a closed bracket, or an OR operator.\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m tokenIdx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(tokensStream) \u001b[38;5;129;01mand\u001b[39;00m tokensStream[tokenIdx]\u001b[38;5;241m.\u001b[39mtokenType \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [TokenTypes\u001b[38;5;241m.\u001b[39mOR, TokenTypes\u001b[38;5;241m.\u001b[39mClosedBracket]:\n\u001b[1;32m---> 19\u001b[0m     rightOperand, tokenIdx \u001b[38;5;241m=\u001b[39m \u001b[43mparseQuantified\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokensStream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenIdx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# token index is incremented here implecitly.\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     leftOperand \u001b[38;5;241m=\u001b[39m SequenceOfCharactersNode(leftOperand, rightOperand) \u001b[38;5;66;03m# recursive assignment, cascading the elemets in the leftOperand.\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m leftOperand, tokenIdx\n",
      "Input \u001b[1;32mIn [75]\u001b[0m, in \u001b[0;36mparseQuantified\u001b[1;34m(tokensStream, tokenIdx)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m    This function is responsible for parsing the quantified cases.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m    it implements this grammar: \u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m        parseQuantified -> parseBase (+ | * | ?)?\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# first we assume that we have only one operand \u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m leftOperand, tokenIdx \u001b[38;5;241m=\u001b[39m parseBase(tokensStream, tokenIdx)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# now we need to check if there are more operands \u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenIdx \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(tokensStream):\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m# no more operands, just return the leftOperand \u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "# Its time to test our logic. \n",
    "# regex = 'ab*c+de?(f|g|h)|mr|n|[pq]'\n",
    "# regex = 'a+'\n",
    "# tokens = Lexer.lexReg(regex)\n",
    "# ast = parse(tokens.tokenStream)\n",
    "# print(ast)\n",
    "toks = [\n",
    "    Token(TokenTypes.LiteralChar,\"a\"),\n",
    "    Token(TokenTypes.LiteralChar,\"b\"),\n",
    "    Token(TokenTypes.Astrisk,\"*\"),\n",
    "    Token(TokenTypes.LiteralChar,\"c\"),\n",
    "    Token(TokenTypes.Plus,\"+\"),\n",
    "    Token(TokenTypes.LiteralChar,\"d\"),\n",
    "    Token(TokenTypes.LiteralChar,\"e\"),\n",
    "    Token(TokenTypes.QuestionMark,\"?\"),\n",
    "\n",
    "    Token(TokenTypes.OpenBracket,\"(\"),\n",
    "    Token(TokenTypes.LiteralChar,\"f\"),\n",
    "    Token(TokenTypes.OR,\"|\"),\n",
    "    Token(TokenTypes.LiteralChar,\"g\"),\n",
    "    Token(TokenTypes.OR,\"|\"),\n",
    "    Token(TokenTypes.LiteralChar,\"h\"),\n",
    "    Token(TokenTypes.ClosedBracket,\")\"),\n",
    "\n",
    "    Token(TokenTypes.OR,\"|\"),\n",
    "    Token(TokenTypes.LiteralChar,\"m\"),\n",
    "    Token(TokenTypes.LiteralChar,\"r\"),\n",
    "\n",
    "    Token(TokenTypes.OR,\"|\"),\n",
    "    Token(TokenTypes.LiteralChar,\"n\"),\n",
    "\n",
    "    Token(TokenTypes.OpenBracket,\"[\"),\n",
    "    Token(TokenTypes.LiteralChar,\"p\"),\n",
    "    Token(TokenTypes.LiteralChar,\"q\"),\n",
    "    Token(TokenTypes.ClosedBracket,\"]\")\n",
    "]\n",
    "print(parse(toks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
