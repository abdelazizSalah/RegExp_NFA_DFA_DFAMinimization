{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexer\n",
    "* This Component is responsible for breaking the input into tokens.\n",
    "* And it escape all spaces and newlines.\n",
    "* and we implemented it mainly for 2 reasons:\n",
    "    1. We will need to do parsing, and parsing on raw characters is ugly\n",
    "\n",
    "    2. (Optional) We will need to handle escapes :\n",
    "     Suppose you want to match the literal string '(', how\n",
    "     would you do it ?\n",
    "     If your regex engine doesn't handle escapes, you\n",
    "     can't, but with escapes it's simply the regex \"\\(\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First of all we need to define all the tokens' types\n",
    "DEBUG = True\n",
    "from enum import Enum\n",
    "class TokenTypes(Enum):\n",
    "    OR = 1 \n",
    "    Astrisk = 2 \n",
    "    Plus = 3 \n",
    "    QuestionMark = 4\n",
    "    OpenSquareBracket = 5 \n",
    "    ClosedSquareBracket = 6\n",
    "    OpenBracket = 7\n",
    "    ClosedBracket = 8\n",
    "    Hiphen = 9 \n",
    "    Dot = 10\n",
    "    LiteralChar = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we need to define a class for the tokens \n",
    "class Token: \n",
    "    def __init__(self, tokenType, value):\n",
    "        self.tokenType = tokenType\n",
    "        self.value = value # the string value of the token, and is used in case of LiteralChar.\n",
    "\n",
    "# we need to define a class for token stream, and a pointer, which is initialized to 0, because they will be used later by the \n",
    "class TokenStream: \n",
    "    def __init__ (self, tokenStream = [],tokenPointer=0): \n",
    "        self.tokenStream = tokenStream\n",
    "        self.tokenPointer = tokenPointer\n",
    "    def resetStream (self): \n",
    "        self.tokenStream = []\n",
    "        self.tokenPointer = 0\n",
    "    def advanceTokenPointer(self):\n",
    "        self.tokenPointer += 1\n",
    "    def getCurrentToken(self):\n",
    "        return self.tokenStream[self.tokenPointer]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets implement the Lexer Class \n",
    "class Lexer: \n",
    "    @staticmethod\n",
    "    def lexReg(regex:str): \n",
    "        '''\n",
    "            This function is responsible for lexing the regular expression and returning a list of tokens.\n",
    "            Input: \n",
    "                regex: a string representing the regular expression.\n",
    "            Output:\n",
    "                a list of tokens.\n",
    "        '''\n",
    "        # create a map which maps each character to its corresponding token type.\n",
    "        metaCharactersMap = { \n",
    "            '|': TokenTypes.OR,\n",
    "            '*': TokenTypes.Astrisk,\n",
    "            '+': TokenTypes.Plus,\n",
    "            '?': TokenTypes.QuestionMark,\n",
    "            '[': TokenTypes.OpenSquareBracket,\n",
    "            ']': TokenTypes.ClosedSquareBracket,\n",
    "            '(': TokenTypes.OpenBracket,\n",
    "            ')': TokenTypes.ClosedBracket,\n",
    "            '-': TokenTypes.Hiphen,\n",
    "            '.': TokenTypes.Dot\n",
    "            # LiteralCharacters are any other characters that are not in the map.\n",
    "        }\n",
    "\n",
    "        # define spaceChar and escapeChar\n",
    "        spaceChar = ' '\n",
    "        escapeChar = '\\\\'\n",
    "\n",
    "        # initialize an empty token stream\n",
    "        tokens = TokenStream()\n",
    "        tokens.resetStream()\n",
    "\n",
    "        # we will need to use two pointers approach, one holds the previous character, and the other holds the current character.\n",
    "        previousChar = None\n",
    "        for char in regex : \n",
    "            if char == spaceChar: \n",
    "                continue\n",
    "            # skip all escapes\n",
    "            if char == escapeChar: \n",
    "                # if the previous character is escape character, then we need to add the current character as a LiteralChar token.\n",
    "                if previousChar == escapeChar:\n",
    "                    tokens.tokenStream.append(Token(TokenTypes.LiteralChar, char))\n",
    "                    previousChar = None\n",
    "                    continue\n",
    "                # else, then we are preparing ourself to escape the next character.\n",
    "                previousChar = char\n",
    "                continue\n",
    "            # check if the current character meta character and not preceeded with space \n",
    "            if char in metaCharactersMap and previousChar != escapeChar: \n",
    "                # add the current character to the token stream\n",
    "                tokens.tokenStream.append(Token(metaCharactersMap[char], char))\n",
    "            else: \n",
    "                # in this case we are trying to match the exact symbol, so we need to add it as a LiteralChar token.\n",
    "                tokens.tokenStream.append(Token(TokenTypes.LiteralChar, char))\n",
    "            previousChar = char\n",
    "        return tokens\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TokenTypes.LiteralChar \\\n"
     ]
    }
   ],
   "source": [
    "# Lets Test the Lexer \n",
    "def testLexer(): \n",
    "    # regex = 'a?b(cd|ef)[a-z]'\n",
    "    \n",
    "    regex = '\\\\\\\\' #\\\\\n",
    "    tokens = Lexer.lexReg(regex)\n",
    "    for token in tokens.tokenStream: \n",
    "        print(token.tokenType, token.value)\n",
    "\n",
    "testLexer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
