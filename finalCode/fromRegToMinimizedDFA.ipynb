{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sH0IfLqhrnk_"
      },
      "source": [
        "## Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "PMHmWGr2rnlC"
      },
      "outputs": [],
      "source": [
        "from collections import deque\n",
        "\n",
        "class Validation:\n",
        "    def __init__(self):\n",
        "        print('calling validation')\n",
        "        self.special_char = {char: 1 for char in ['+', '|', '?', '*','-']}\n",
        "        self.alphanumeric_char = {char: 1 for char in 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789.'}\n",
        "        self.paranteses = {char: 1 for char in ['(', ')', '[', ']']}\n",
        "\n",
        "    def empty_expression(self,expression):\n",
        "        '''\n",
        "            this returns true if the given expression is empty.\n",
        "        '''\n",
        "        return len(expression) == 0\n",
        "\n",
        "    def valid_starting_char(self,expression):\n",
        "        '''\n",
        "            this returns false, if the expression starts with special characters, or any undefined characters.\n",
        "        '''\n",
        "        return (expression[0] in self.paranteses) or ( expression[0] in self.alphanumeric_char)\n",
        "\n",
        "    def no_sequence_of_special_char(self,expression):\n",
        "        '''\n",
        "            this returns false if there is any sequence of special characters such as\n",
        "                **\n",
        "                ??\n",
        "                +++\n",
        "                +*\n",
        "                *+\n",
        "                ||\n",
        "                ?+\n",
        "                |?\n",
        "                a+? -> i would prevent such cases\n",
        "                a*? -> i would prevent such cases\n",
        "            take care of these cases:\n",
        "              a+|\n",
        "              a?|\n",
        "             because it is a valid case.\n",
        "\n",
        "        '''\n",
        "        # [[]] handeled in valid parantheses\n",
        "        # (()) allowed\n",
        "        sequenceExist=False\n",
        "        lastChar = ''\n",
        "        for char in expression:\n",
        "            if char in self.special_char :\n",
        "                if sequenceExist:\n",
        "                    if lastChar == char: # any two special characters are not allowed to be repeated.\n",
        "                        return False\n",
        "                    # the only two allowed cases are +| *| +? *?\n",
        "                    if char == '|':\n",
        "                        if lastChar == '-':  # this is the only not allowed case, because we checked above on the ||\n",
        "                            return False\n",
        "                    # if char == '?':\n",
        "                    #     if lastChar in ['|', '-']:\n",
        "                    #         return False\n",
        "                    if char in ['*', '+','?']:\n",
        "                       return False\n",
        "\n",
        "                sequenceExist = True\n",
        "                lastChar = char\n",
        "            else:\n",
        "                sequenceExist = False\n",
        "        return True\n",
        "\n",
        "    def no_end_with_dash_or_pip(self, expression):\n",
        "        if expression[-1] == '-' or expression[-1] == '|':  #new\n",
        "            return False\n",
        "        for i,char in enumerate(expression):\n",
        "            if char == ']' or char == ')':\n",
        "                if expression[i-1] == '-' or expression[i-1] == '|':\n",
        "                    return False\n",
        "            if char == '[' or char == '(':\n",
        "                if expression[i+1] == '-' or expression[i+1] == '|':\n",
        "                    return False\n",
        "        return True\n",
        "\n",
        "    def no_special_char_in_parantheses(self, expression):\n",
        "        rounded_brackets=False\n",
        "        new_special_char = {char: 1 for char in ['+', '|', '?', '*']}\n",
        "        paranthes=False\n",
        "        prev_paranthes=False\n",
        "        for i,char in enumerate(expression):\n",
        "            if rounded_brackets and char == '-' and not paranthes:\n",
        "                return False\n",
        "            if char == '[':\n",
        "                prev_paranthes = True\n",
        "                paranthes = True\n",
        "            elif char == ']':\n",
        "                paranthes = False\n",
        "                prev_paranthes = False\n",
        "            # check () doesn't start with special char      #new\n",
        "            elif char == '(':\n",
        "                paranthes=False\n",
        "                rounded_brackets = True\n",
        "                if(expression[i+1] in self.special_char):\n",
        "                    return False\n",
        "            elif char == ')':\n",
        "                rounded_brackets = False\n",
        "                paranthes=prev_paranthes\n",
        "            if paranthes and char in new_special_char:\n",
        "                return False\n",
        "\n",
        "\n",
        "\n",
        "        return True\n",
        "\n",
        "    def validate_parentheses(self, expression: str) -> bool:\n",
        "        '''\n",
        "        This function validates the parentheses in the expression.\n",
        "        It returns True if the parentheses are valid, otherwise it returns False.\n",
        "        The parantheses are valid in the following cases:\n",
        "            1. no empty parentheses either [] or ()\n",
        "            2. # of '(' == # of ')' and in a correct order, same for [].\n",
        "\n",
        "        Input:\n",
        "            expression: str\n",
        "        Output:\n",
        "            bool\n",
        "        '''\n",
        "        stack = deque()\n",
        "        for i ,char in enumerate(expression):\n",
        "            if char == '(' or char == '[':\n",
        "                if i+1 >= len(expression) or expression[i+1] == ')' or expression[i+1] == ']':\n",
        "                    return False\n",
        "                stack.append(char)\n",
        "            elif char == ')':\n",
        "                if len(stack) == 0 or stack.pop() != '(' :\n",
        "                    return False\n",
        "\n",
        "            elif char == ']':\n",
        "                if len(stack) == 0 or stack.pop() != '[' :\n",
        "                    return False\n",
        "\n",
        "        return len(stack) == 0\n",
        "\n",
        "    def validate_no_neasted_square_brackets(self, expression:str) -> bool:\n",
        "        '''\n",
        "        This function validates the square brackets in the expression.\n",
        "        It returns True if the square brackets are valid, otherwise it returns False.\n",
        "        The square brackets are valid in the following case:\n",
        "            1. no nested square brackets.\n",
        "\n",
        "        Input:\n",
        "            expression: str\n",
        "        Output:\n",
        "            bool\n",
        "        '''\n",
        "        stack = deque()\n",
        "        for char in expression:\n",
        "            if char == '[':\n",
        "                if len(stack) != 0: # this mean that there is a nested square bracket.\n",
        "                    return False\n",
        "                stack.append(char)\n",
        "            elif char == ']':\n",
        "                if len(stack) == 0 or stack.pop() != '[':\n",
        "                    return False\n",
        "        return len(stack) == 0\n",
        "\n",
        "    def validate_regular_expression(self, expression: str) -> bool:\n",
        "        '''\n",
        "            This is the main validation function, which calls all the above utility validation functions.\n",
        "\n",
        "            Sequence of validation:\n",
        "                1. empty\n",
        "                2. valid starting character\n",
        "                3. parentheses\n",
        "                    3.1. no empty parentheses and valid parentheses.\n",
        "                    3.2. no nested square brackets\n",
        "                4. no special characters inside the parentheses.\n",
        "                5. no sequence of special characters\n",
        "                6. no end with dash or pipe\n",
        "\n",
        "\n",
        "\n",
        "        '''\n",
        "        if self.empty_expression(expression):\n",
        "            return False\n",
        "\n",
        "        if not self.valid_starting_char(expression):\n",
        "            return False\n",
        "\n",
        "        if not self.validate_parentheses(expression):\n",
        "            return False\n",
        "\n",
        "        if not self.validate_no_neasted_square_brackets(expression):\n",
        "            return False\n",
        "\n",
        "        if not self.no_special_char_in_parantheses(expression):\n",
        "            return False\n",
        "\n",
        "        if not self.no_sequence_of_special_char(expression):\n",
        "            return False\n",
        "\n",
        "        if not self.no_end_with_dash_or_pip(expression):\n",
        "            return False\n",
        "\n",
        "        # it passed all tests.\n",
        "        print(\"Passed all test cases ðŸ¥‚ðŸ¥³\")\n",
        "        return True\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kP4GcpqrnlF"
      },
      "source": [
        "## (1) Lexer\n",
        "* This Component is responsible for breaking the input into tokens.\n",
        "* And it escape all spaces and newlines.\n",
        "* and we implemented it mainly for 2 reasons:\n",
        "    1. We will need to do parsing, and parsing on raw characters is ugly\n",
        "\n",
        "    2. (Optional) We will need to handle escapes :\n",
        "     Suppose you want to match the literal string '(', how\n",
        "     would you do it ?\n",
        "     If your regex engine doesn't handle escapes, you\n",
        "     can't, but with escapes it's simply the regex \"\\(\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "gmrsGsd3rnlG"
      },
      "outputs": [],
      "source": [
        "# First of all we need to define all the tokens' types\n",
        "DEBUG = True\n",
        "from enum import Enum\n",
        "class TokenTypes(Enum):\n",
        "    OR = 1\n",
        "    Astrisk = 2\n",
        "    Plus = 3\n",
        "    QuestionMark = 4\n",
        "    OpenSquareBracket = 5\n",
        "    ClosedSquareBracket = 6\n",
        "    OpenBracket = 7\n",
        "    ClosedBracket = 8\n",
        "    Hiphen = 9\n",
        "    Dot = 10\n",
        "    LiteralChar = 11"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "c1xLHqY2rnlG"
      },
      "outputs": [],
      "source": [
        "# now we need to define a class for the tokens\n",
        "class Token:\n",
        "    def __init__(self, tokenType, value):\n",
        "        self.tokenType = tokenType\n",
        "        self.value = value # the string value of the token, and is used in case of LiteralChar.\n",
        "\n",
        "# we need to define a class for token stream, and a pointer, which is initialized to 0, because they will be used later by the\n",
        "class TokenStream:\n",
        "    def __init__ (self, tokenStream = [],tokenPointer=0):\n",
        "        self.tokenStream = tokenStream\n",
        "        self.tokenPointer = tokenPointer\n",
        "    def resetStream (self):\n",
        "        self.tokenStream = []\n",
        "        self.tokenPointer = 0\n",
        "    def advanceTokenPointer(self):\n",
        "        self.tokenPointer += 1\n",
        "    def getCurrentToken(self):\n",
        "        return self.tokenStream[self.tokenPointer]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "oujFMXkyrnlG"
      },
      "outputs": [],
      "source": [
        "# Now lets implement the Lexer Class\n",
        "class Lexer:\n",
        "    @staticmethod\n",
        "    def lexReg(regex:str):\n",
        "        '''\n",
        "            This function is responsible for lexing the regular expression and returning a list of tokens.\n",
        "            Input:\n",
        "                regex: a string representing the regular expression.\n",
        "            Output:\n",
        "                a list of tokens.\n",
        "        '''\n",
        "        # create a map which maps each character to its corresponding token type.\n",
        "        conversionMap = {\n",
        "            '|': TokenTypes.OR,\n",
        "            '*': TokenTypes.Astrisk,\n",
        "            '+': TokenTypes.Plus,\n",
        "            '?': TokenTypes.QuestionMark,\n",
        "            '[': TokenTypes.OpenSquareBracket,\n",
        "            ']': TokenTypes.ClosedSquareBracket,\n",
        "            '(': TokenTypes.OpenBracket,\n",
        "            ')': TokenTypes.ClosedBracket,\n",
        "            '-': TokenTypes.Hiphen,\n",
        "            '.': TokenTypes.Dot\n",
        "            # LiteralCharacters are any other characters that are not in the map.\n",
        "        }\n",
        "\n",
        "        # define spaceChar and escapeChar\n",
        "        spaceChar = ' '\n",
        "        escapeChar = '\\\\'\n",
        "\n",
        "        # initialize an empty token stream\n",
        "        tokens = TokenStream()\n",
        "        tokens.resetStream()\n",
        "\n",
        "        # we will need to use two pointers approach, one holds the previous character, and the other holds the current character.\n",
        "        previousChar = None\n",
        "        for char in regex :\n",
        "            # skip all spaces\n",
        "            if char == spaceChar:\n",
        "                continue\n",
        "            # mark escapes\n",
        "            if char == escapeChar:\n",
        "                # if the previous character is escape character, then we need to add the current character as a LiteralChar token.\n",
        "                if previousChar == escapeChar:\n",
        "                    tokens.tokenStream.append(Token(TokenTypes.LiteralChar, char))\n",
        "                    # clear the mark.\n",
        "                    previousChar = None\n",
        "                    continue\n",
        "                # else, then we are preparing ourself to escape the next character.\n",
        "                previousChar = char\n",
        "                continue\n",
        "            # check if the current character is in conversionMap and not preceeded with space\n",
        "            if char in conversionMap and previousChar != escapeChar:\n",
        "                # add the current character to the token stream\n",
        "                tokens.tokenStream.append(Token(conversionMap[char], char))\n",
        "            else:\n",
        "                # in this case we are trying to match the exact symbol, so we need to add it as a LiteralChar token.\n",
        "                tokens.tokenStream.append(Token(TokenTypes.LiteralChar, char))\n",
        "            previousChar = char\n",
        "        return tokens\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPyMHqxvrnlI",
        "outputId": "1b9119be-b1c0-4e0f-d3d8-3f9430fd82ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TokenTypes.LiteralChar a\n",
            "TokenTypes.LiteralChar b\n",
            "TokenTypes.Astrisk *\n",
            "TokenTypes.LiteralChar c\n",
            "TokenTypes.Plus +\n",
            "TokenTypes.LiteralChar d\n",
            "TokenTypes.LiteralChar e\n",
            "TokenTypes.QuestionMark ?\n",
            "TokenTypes.OpenBracket (\n",
            "TokenTypes.LiteralChar f\n",
            "TokenTypes.OR |\n",
            "TokenTypes.LiteralChar g\n",
            "TokenTypes.OR |\n",
            "TokenTypes.LiteralChar h\n",
            "TokenTypes.ClosedBracket )\n",
            "TokenTypes.OR |\n",
            "TokenTypes.LiteralChar m\n",
            "TokenTypes.LiteralChar r\n",
            "TokenTypes.OR |\n",
            "TokenTypes.LiteralChar n\n",
            "TokenTypes.OR |\n",
            "TokenTypes.OpenSquareBracket [\n",
            "TokenTypes.LiteralChar p\n",
            "TokenTypes.LiteralChar q\n",
            "TokenTypes.ClosedSquareBracket ]\n"
          ]
        }
      ],
      "source": [
        "# Lets Test the Lexer\n",
        "def testLexer():\n",
        "    # regex = 'a?b(cd|ef)[a-z]'\n",
        "\n",
        "    regex = 'ab*c+de?(f|g|h)|mr|n|[pq]' #\\\\\n",
        "    tokens = Lexer.lexReg(regex)\n",
        "    for token in tokens.tokenStream:\n",
        "        print(token.tokenType, token.value)\n",
        "\n",
        "testLexer()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jr2hqzvXrnlI"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hD1rsOxqrnlI"
      },
      "source": [
        "# (2) Regex_Parser"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49lk1132rnlI"
      },
      "source": [
        "* Parser is a program that takes a sequence of tokens and produces a parse tree.\n",
        "* but why do we need a parse tree?\n",
        "* The parse tree is a visual representation of the syntax of a program. It shows the structure of the program and how the different parts of the program relate to each other.\n",
        "* Moreover, it allow us to keep the precedence of the elements in the expression, consider this case:\n",
        "  * ![PrecedensMatters](attachment:image.png)\n",
        "* In this case, if we used the naive approach, and moved from the beginning of the string, and just apply operations as we detects it, we will get a wrong result, because we shouldn't add 1 to 2, aw 2 should be firstly multiplied by its rightsside.\n",
        "* If we handled it as a string, we will make mistakes for sure.\n",
        "* So now it makes sense to implement a parser, but how to do that?\n",
        "* We have an algorithm called Recursive Descent Parser, which is a top-down parser that uses a set of recursive procedures to process the input.\n",
        "* its steps are as follows:\n",
        "  * Define your AST-Node el node 3ndk shaklha 3aml ezay -> pointer msln w character w hakaza.\n",
        "  * b3den le kol operation 3ndk, m7tag te3ml class by-inheret mn el AST Node de.\n",
        "  * ie: Or-AST-Node should be a node with two children, left and right, and its logic is that we consider only one of the children to be true, and the other is false.\n",
        "  * ie: Sequence-AST-Node should be a node with two children, left and right, and its logic is that we should execute the left child first, then the right child.\n",
        "  * and so on till we cover all the cases\n",
        "  * then define a leaf node -> Literal-Character-AST nodes, which is a node that has a value, and it is the end of the tree.\n",
        "* After defining our hirarechal datastructure, we need to define a grammar to parse\n",
        "* This is a method called Backus-Naur\n",
        "  * ![OurGrammar](attachment:image-2.png)\n",
        "  * Our grammar should be in such hirarechal way.\n",
        "      * we regex-expression : regex_or_expression\n",
        "      * regex-or-expr ::= regex-seq-expr (OR_TOKEN regex-seq-expr)*\n",
        "      * regex-seq-expr ::= regex-quantified-expr (regex-quantified-expr)*\n",
        "      * regex-quantified-expr ::= regex-base-expr (STAR_TOKEN | PLUS_TOKEN | QUESTION_MARK_TOKEN)?\n",
        "      * regex-base-expr ::= LITERAL_CHAR_TOKEN  | [OPEN_SQUARE_BRACKET_TOKEN  square-bracket-content CLOSED_SQUARE_BRACKET_TOKEN ]| (OPEN_PARENTHESIS_TOKEN regex-expr CLOSED_PARENTHESIS_TOKEN )\n",
        "      * square-bracket-content ::= square-bracket-element+\n",
        "      * square-bracket-element ::= LITERAL_CHAR_TOKEN\n",
        "      * | LITERAL_CHAR_TOKEN DASH LITERAL_CHAR_TOKEN\n",
        "* Now we need a code to implement this, Recursive descent says that, it is very easy to implement this grammar, by defining a function for each non-terminal in the grammar, and then call the functions recursively to parse the input.\n",
        "* steps:\n",
        "  1. htbd2 t3ml function le kol element fe el grammar, ie: ehna olna 3ndna regex-exepression, yeb2a te3ml function esmha keda, w t5leha tnfzz bzbt elly enta katbu, fa keda el regExp => regex_or_expr(regex)\n",
        "  2. w b3den tbd2 t3ml implementation lel regex_or_expr bnfs el tre2a, w tfdl keda l7d ma tewsl lel final literals."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsbmsfV7rnlJ"
      },
      "source": [
        "# Procedures flow\n",
        "![Flow](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bP83o87ernlJ"
      },
      "source": [
        "### Enough Talking, lets start implementing the code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "1TgWLMpPrnlJ"
      },
      "outputs": [],
      "source": [
        "# First we need to define our AST Nodes\n",
        "class AstNode:\n",
        "    pass # this is an abstract class\n",
        "\n",
        "'''\n",
        "    What are our cases?\n",
        "        1. + -> One or more node class\n",
        "        2. * -> Zero or more node class\n",
        "        3. ? -> Zero or one class\n",
        "        4. | -> OR class\n",
        "        5. [] -> set of characters class\n",
        "        6. abcd -> Sequence of characters class\n",
        "        7. LiteralChar -> Literal Character class\n",
        "'''\n",
        "class PlusNode(AstNode):\n",
        "    left: AstNode\n",
        "    val: str\n",
        "    def __init__(self, left):\n",
        "        self.left = left\n",
        "        self.val = '+'\n",
        "\n",
        "class AstriskNode(AstNode):\n",
        "    left: AstNode\n",
        "    val: str\n",
        "    def __init__(self, left):\n",
        "        self.left = left\n",
        "        self.val = '*'\n",
        "\n",
        "\n",
        "class QuestionMarkNode(AstNode):\n",
        "    left: AstNode\n",
        "    val: str\n",
        "    def __init__(self, left):\n",
        "        self.left = left\n",
        "        self.val = '?'\n",
        "class OrNode(AstNode):\n",
        "    left: AstNode\n",
        "    right: AstNode\n",
        "    val:str\n",
        "    def __init__(self, left, right):\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "        self.val = '|'\n",
        "\n",
        "class SetOfCharactersNode(AstNode):\n",
        "    val:str\n",
        "    characters: set  # maybe strings or pairs as in case of [0-9]\n",
        "    def __init__(self, characters):\n",
        "        self.characters = characters\n",
        "        self.val = '&'\n",
        "\n",
        "class SequenceOfCharactersNode(AstNode):\n",
        "    left: AstNode\n",
        "    right: AstNode\n",
        "    val: str\n",
        "    def __init__(self, left, right):\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "        self.val = '&'\n",
        "\n",
        "class LiteralCharNode(AstNode):\n",
        "    value: str\n",
        "    def __init__(self, value):\n",
        "        self.value = value\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JWemHburnlK"
      },
      "source": [
        "### now lets define our grammar\n",
        "\n",
        "1. parse -> parseReg\n",
        "2. parseReg -> parseOr\n",
        "2. parseOr -> parseSeq (| parseSeq)*\n",
        "2. parseSeq -> parseQuantified (parseQuantified)*\n",
        "2. parseQuantified -> parseBase (+ | * | ?)?\n",
        "2. parseBase -> LiteralChar | SetOfCharacters | ( parseReg )\n",
        "2. so we just need to implement one function for each of those following the recursive descent parsing approach.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "407d3XEVrnlK"
      },
      "outputs": [],
      "source": [
        "# 1. parseBase -> because it is our basecase, so we must implement the functions bottom up.\n",
        "def parseSquareBrackets(tokenStream, tokenIdx):\n",
        "    '''\n",
        "        Our content may have different patterns:\n",
        "            1. a-z -> one range\n",
        "            2. abc0-9 -> certain Literals and a range\n",
        "            3. xyz -> certain Literals\n",
        "            4. A-Z0-9 multiple Ranges\n",
        "            5. all above.\n",
        "        how to handle them?\n",
        "        1. any range should be treated as a tuple of 2 characters\n",
        "            first is the starting index\n",
        "            second is the ending index.\n",
        "        2. any sequence of characters\n",
        "    '''\n",
        "    characters = []\n",
        "    prevIsDash = False\n",
        "    # We Should iterate till we find a ] <=\n",
        "    while tokenStream[tokenIdx].tokenType != TokenTypes.ClosedSquareBracket:\n",
        "        if tokenStream[tokenIdx].tokenType == TokenTypes.Hiphen:\n",
        "            prevIsDash = True\n",
        "        elif prevIsDash:\n",
        "            # get the last appended char as the starting character\n",
        "            startingChar = characters.pop()\n",
        "            # get the current character as an ending character\n",
        "            endingChar = tokenStream[tokenIdx].value\n",
        "            # push in the characters a tuple\n",
        "            characters.append((startingChar, endingChar))\n",
        "            # set the prevIsDash back to False\n",
        "            prevIsDash = False\n",
        "        elif tokenStream[tokenIdx].tokenType == TokenTypes.LiteralChar:\n",
        "            characters.append(tokenStream[tokenIdx].value)\n",
        "        tokenIdx += 1\n",
        "\n",
        "    # here we should assign it to set, but I left it as List.\n",
        "    return SetOfCharactersNode(characters), tokenIdx\n",
        "\n",
        "def parseBase(tokenStream, tokenIdx):\n",
        "    '''\n",
        "        This function is responsible for parsing the base cases, or applying a recursive call on bracktes\n",
        "        Implmenting this grammar:  parseBase -> LiteralChar | SetOfCharacters | ( parseReg )\n",
        "\n",
        "        Input:\n",
        "            tokenStream: a list of tokens.\n",
        "            tokenIdx: the current index of the token.\n",
        "        Output:\n",
        "            an AST node, and the new index\n",
        "    '''\n",
        "\n",
        "    # we have three cases, LiteralChar, SetOfCharacters, and ( parseReg )\n",
        "\n",
        "    # Extracting the token\n",
        "    token:Token = tokenStream[tokenIdx]\n",
        "    tokenIdx += 1\n",
        "\n",
        "\n",
        "    # LiteralChar\n",
        "    if token.tokenType == TokenTypes.LiteralChar:\n",
        "        # print('character found' + token.value)\n",
        "        # print('its index is ' + str(tokenIdx))\n",
        "        node, idx = LiteralCharNode(token.value), tokenIdx\n",
        "        # print(node, idx)\n",
        "\n",
        "        return node, idx\n",
        "\n",
        "    # set of Characters.\n",
        "    if token.tokenType == TokenTypes.OpenSquareBracket:\n",
        "        # print('Parsing Square Brackets')\n",
        "        sq, idx = parseSquareBrackets(tokenStream, tokenIdx)\n",
        "        # we need a utility function to parse the data inside the square brackets.\n",
        "        # print(sq, idx)\n",
        "        return sq, idx+1 # we need to skip the closed square bracket.\n",
        "\n",
        "    if token.tokenType == TokenTypes.OpenBracket:\n",
        "        # print('Recursion')\n",
        "        # we need to parse the regular expression inside the brackets.\n",
        "        parsedReg, tokenIdx =  parseRegex(tokenStream, tokenIdx)\n",
        "        # print('Recursion Done')\n",
        "        # print(parsedReg, tokenIdx + 1)\n",
        "        return parsedReg, tokenIdx+1 # we need to skip the closed bracket.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "HAD-GfOXrnlL"
      },
      "outputs": [],
      "source": [
        "def parseQuantified(tokensStream, tokenIdx):\n",
        "    '''\n",
        "        This function is responsible for parsing the quantified cases.\n",
        "        it implements this grammar:\n",
        "            parseQuantified -> parseBase (+ | * | ?)?\n",
        "    '''\n",
        "\n",
        "    # first we assume that we have only one operand\n",
        "    leftOperand, tokenIdx = parseBase(tokensStream, tokenIdx)\n",
        "    # print(leftOperand, tokenIdx)\n",
        "\n",
        "    # now we need to check if there are more operands\n",
        "    if tokenIdx >= len(tokensStream):\n",
        "        # no more operands, just return the leftOperand\n",
        "        return leftOperand, tokenIdx\n",
        "\n",
        "    # now we have to check on the operator (*,+,?)\n",
        "    token = tokensStream[tokenIdx]\n",
        "    # print('the current token is: ' + token.value)\n",
        "    '''\n",
        "        el moshkela btb2a hena, enu byb2a shayef b, w de msh btb2a wla haga mn el t7t dol\n",
        "    '''\n",
        "    if token.tokenType == TokenTypes.QuestionMark:\n",
        "        return QuestionMarkNode(leftOperand), tokenIdx + 1\n",
        "    if token.tokenType == TokenTypes.Plus:\n",
        "        return PlusNode(leftOperand), tokenIdx + 1\n",
        "    if token.tokenType == TokenTypes.Astrisk:\n",
        "        return AstriskNode(leftOperand), tokenIdx + 1\n",
        "\n",
        "    # if we reached here, this implies that we finished the quantifiers.\n",
        "    return leftOperand, tokenIdx\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "xhYyGOclrnlM"
      },
      "outputs": [],
      "source": [
        "def parseSequence(tokensStream, tokenIdx):\n",
        "    '''\n",
        "        This function is responsible for parsing the sequence of characters.\n",
        "        it implements this grammar:\n",
        "            parseSequence -> parseQuantified (parseQuantified)*\n",
        "    '''\n",
        "    # first we assume that we have only one operand\n",
        "    leftOperand, tokenIdx = parseQuantified(tokensStream, tokenIdx)\n",
        "\n",
        "    # now we need to check if we have more elements\n",
        "    if tokenIdx >= len(tokensStream):\n",
        "        # no more elements\n",
        "        return leftOperand, tokenIdx\n",
        "\n",
        "    # we need to continue parsing all sequence like abcd, or if we have operator we should also stop.\n",
        "    # to do so, we have to have a stoping condition\n",
        "    # we will stop if we have a closed bracket, or an OR operator.\n",
        "    while tokenIdx < len(tokensStream) and tokensStream[tokenIdx].tokenType not in [TokenTypes.OR, TokenTypes.ClosedBracket]:\n",
        "        # print(tokensStream[tokenIdx].tokenType)\n",
        "        # print(tokenIdx)\n",
        "        rightOperand, tokenIdx = parseQuantified(tokensStream, tokenIdx) # token index is incremented here implecitly.\n",
        "        leftOperand = SequenceOfCharactersNode(left=leftOperand,right= rightOperand) # recursive assignment, cascading the elemets in the leftOperand.\n",
        "        # print(tokenIdx)\n",
        "    return leftOperand, tokenIdx\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "YeetK3wjrnlM"
      },
      "outputs": [],
      "source": [
        "def parseOr (tokensStream, tokenIdx):\n",
        "    '''\n",
        "        This function is responsible for parsing the OR operator.\n",
        "        it implements this grammar:\n",
        "            parseOr -> parseSequence (| parseSequence)*\n",
        "    '''\n",
        "    # first we assume that we have only one operand\n",
        "    leftOperand, tokenIdx = parseSequence(tokensStream, tokenIdx)\n",
        "\n",
        "    # now we need to check if we have more elements\n",
        "    if tokenIdx >= len(tokensStream):\n",
        "        return leftOperand, tokenIdx\n",
        "\n",
        "    # now we still have elements, we just need to do recursion, as we see or operator\n",
        "    while tokenIdx < len(tokensStream) and tokensStream[tokenIdx].tokenType == TokenTypes.OR:\n",
        "        rightOperand, tokenIdx = parseSequence(tokensStream, tokenIdx+1)\n",
        "        leftOperand = OrNode(left =leftOperand,right= rightOperand)\n",
        "\n",
        "    return leftOperand, tokenIdx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "3eC0I4g7rnlM"
      },
      "outputs": [],
      "source": [
        "def parseRegex (tokensStream, index):\n",
        "    '''\n",
        "        This function should implement the grammar:\n",
        "            parseReg -> parseOr\n",
        "    '''\n",
        "    return parseOr(tokensStream, index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "2YfczXURrnlM"
      },
      "outputs": [],
      "source": [
        "def parse(tokensStream):\n",
        "    expression, _ = parseRegex(tokensStream, 0)\n",
        "    return expression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "caZQbbuYrnlM",
        "outputId": "ee7966a3-fbc3-4bd6-e9fe-19b3db70da06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "            A\n",
            "\t\t\t<__main__.PlusNode object at 0x000001EA58A61F70>\n",
            "\t\t<__main__.SequenceOfCharactersNode object at 0x000001EA592DD880>\n",
            "            B\n",
            "\t\t\t<__main__.AstriskNode object at 0x000001EA592DD9A0>\n",
            "\t<__main__.QuestionMarkNode object at 0x000001EA592DDB50>\n",
            "<__main__.SequenceOfCharactersNode object at 0x000001EA592DDEB0>\n",
            "      C\n",
            "\t<__main__.OrNode object at 0x000001EA592DDBE0>\n",
            "      D\n"
          ]
        }
      ],
      "source": [
        "# Its time to test our logic.\n",
        "# regex = 'ab*c+de?(f|g|h)|mr|n|[pq]'\n",
        "# regex = 'a+b' # this is similar to this (a+ . b) so it should be => a+b.\n",
        "regex = '(A+B*)?(C|D)'\n",
        "tokens = Lexer.lexReg(regex)\n",
        "ast:AstNode = parse(tokens.tokenStream)\n",
        "\n",
        "# print a tree from left to right\n",
        "def printTree(node, level=0):\n",
        "    if node is None:\n",
        "        return\n",
        "    if node.__class__.__name__ == 'SetOfCharactersNode':\n",
        "        print('   '*level + str(node.characters))\n",
        "        return\n",
        "    if node.__class__.__name__ == 'LiteralCharNode':\n",
        "        print('   '*level + node.value)\n",
        "        return\n",
        "    if(node.left):\n",
        "       printTree(node.left, level+1)\n",
        "    print('\\t' * level + str(node))\n",
        "    # print('   '*level + str(node.__class__.__name__))\n",
        "    if node.__class__.__name__ in ['AstriskNode', 'PlusNode', 'QuestionMarkNode']:\n",
        "        # all of these has no right operands.\n",
        "        return\n",
        "    if(node.right):\n",
        "        printTree(node.right, level+1)\n",
        "\n",
        "printTree(ast)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gal5hSfLrnlN"
      },
      "source": [
        "### Converting recursive descent to postfix string"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFxXAapVrnlN"
      },
      "source": [
        "### Conversion algorithm:\n",
        "1. function convertToPostfix(node):\n",
        "   1. if node is operand:\n",
        "      1.  output(node.value)\n",
        "    2. else if node is operator:\n",
        "       1. for each child in node.children:\n",
        "            1. convertToPostfix(child)\n",
        "        2. output(node.value)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lu84EJH9rnlN",
        "outputId": "5f302bf9-182a-4bbd-95c7-f89e8144450b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A+B*&?CD|&\n"
          ]
        }
      ],
      "source": [
        "postfixString = ''\n",
        "def convertRecursiveDescentToPostfix (root: AstNode, postfixString:str):\n",
        "    # if no node.\n",
        "    if root == None:\n",
        "        return postfixString\n",
        "\n",
        "    # if it was terminal character\n",
        "    if root.__class__.__name__ == 'LiteralCharNode':\n",
        "        postfixString += root.value\n",
        "        return postfixString\n",
        "\n",
        "    # if set of characters, add them as or operands\n",
        "    if root.__class__.__name__ == 'SetOfCharactersNode':\n",
        "        # InsertedBefore = False\n",
        "        notFirstChar = False\n",
        "        for element in root.characters: # in case of range => [0-9] and so on.\n",
        "            if type(element) == tuple:\n",
        "                startRange = element[0]\n",
        "                endRange = element[1]\n",
        "                # now I need to iterate from the start range to the end range, and add these elements as or\n",
        "                for i in range(ord(startRange), ord(endRange)+1):\n",
        "\n",
        "                    postfixString += chr(i)\n",
        "                    if notFirstChar:\n",
        "                        postfixString+= '|'\n",
        "                    else:\n",
        "                        notFirstChar = True\n",
        "                        # InsertedBefore = True\n",
        "\n",
        "\n",
        "            else:\n",
        "                postfixString += element\n",
        "                if notFirstChar:\n",
        "                    postfixString+= '|'\n",
        "                else:\n",
        "                    notFirstChar = True\n",
        "                    # InsertedBefore = True\n",
        "                # postfixString += element + '|'\n",
        "\n",
        "        # remove last character, it is excess (|)\n",
        "        # postfixString = postfixString[:-1]\n",
        "        return postfixString\n",
        "\n",
        "    # if it was an Sequence or OR node\n",
        "    if root.__class__.__name__ in ['OrNode', 'SequenceOfCharactersNode']:\n",
        "        # apply dfs, left first\n",
        "        postfixString = convertRecursiveDescentToPostfix(root.left, postfixString)\n",
        "        # then right\n",
        "        postfixString = convertRecursiveDescentToPostfix(root.right, postfixString)\n",
        "        postfixString += root.val\n",
        "        return postfixString\n",
        "\n",
        "    # if it was any operator\n",
        "    if root.__class__.__name__ in ['AstriskNode', 'PlusNode', 'QuestionMarkNode']:\n",
        "        # append the character first\n",
        "        postfixString = convertRecursiveDescentToPostfix(root.left, postfixString)\n",
        "        # then add the operatot.\n",
        "        postfixString += root.val\n",
        "        return postfixString\n",
        "\n",
        "postfixString = convertRecursiveDescentToPostfix(ast, postfixString)\n",
        "print(postfixString)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "a_ohdvw-rnlN"
      },
      "outputs": [],
      "source": [
        "def fromRegToPostfix(regex:str) -> str:\n",
        "    # 1. Validate the regex\n",
        "    validator = Validation()\n",
        "    if not validator.validate_regular_expression(regex):\n",
        "        # throw an exception\n",
        "        return 'Invalid Regex'\n",
        "\n",
        "    # 2. Lexe it\n",
        "    tokens = Lexer.lexReg(regex)\n",
        "\n",
        "    # 3. Parse\n",
        "    ast:AstNode = parse(tokens.tokenStream)\n",
        "    postfixString = ''\n",
        "\n",
        "    # 4. convert to Postfix\n",
        "    postfixString = convertRecursiveDescentToPostfix(ast, postfixString)\n",
        "    return postfixString\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Tz3A_IjnrnlO"
      },
      "outputs": [],
      "source": [
        "# assume @ is the epsilon symbol\n",
        "class edge:\n",
        "    def __init__(self,src,dist,label=\"@\"):\n",
        "        self.src = src\n",
        "        self.dist = dist\n",
        "        self.label = label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "hJTJ-yEzrnlO"
      },
      "outputs": [],
      "source": [
        "sid=0\n",
        "\n",
        "class state:\n",
        "    def __init__(self,edges, isTerminating = False, isStart= False):\n",
        "        global sid\n",
        "        self.Outedges:list[edge] = edges\n",
        "        self.label = \"S\"+str(sid)\n",
        "        self.isTerminating = isTerminating\n",
        "        self.isStart = isStart\n",
        "        sid+=1\n",
        "    def add_edge(self,edge):\n",
        "        self.Outedges.append(edge)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "957JFXS1rnlO"
      },
      "outputs": [],
      "source": [
        "from graphviz import Digraph\n",
        "\n",
        "class NFA:\n",
        "    def __init__(self,start,accept,inner_states):\n",
        "        self.start:state = start\n",
        "        self.accept:state = accept\n",
        "        self.inner_states:list[state] =inner_states\n",
        "        self.allStates={}\n",
        "\n",
        "    def display(self):\n",
        "        with open('nfa.json', 'w') as f:\n",
        "            f.write(\"{\\n\")\n",
        "            f.write(\"\\\"startingState\\\":\\\"\"+self.start.label+\"\\\",\\n\")\n",
        "            for s in self.inner_states:\n",
        "                f.write(\"\\\"\"+s.label+\"\\\":\\n\")\n",
        "                f.write(\"   {\\n\")\n",
        "                f.write(\"       \\\"isTerminatingState\\\":\\\"\"+str(s.label==self.accept.label)+\"\\\"\")\n",
        "                if len(s.Outedges)>0:\n",
        "                    f.write(\",\\n\")\n",
        "                for e in s.Outedges:\n",
        "                    f.write(\"       \\\"\"+e.label+\"\\\":\\\"\"+e.dist.label+\"\\\"\")\n",
        "                    if e!=s.Outedges[-1]:\n",
        "                        f.write(\",\\n\")\n",
        "                    else:\n",
        "                        f.write(\"\\n\")\n",
        "                f.write(\"   }\")\n",
        "                if s!=self.inner_states[-1]:\n",
        "                    f.write(\",\\n\")\n",
        "                else:\n",
        "                    f.write(\"\\n\")\n",
        "            f.write(\"}\\n\")\n",
        "\n",
        "    def graph (self):\n",
        "        dot = Digraph(comment='NFA graph',format='png',graph_attr={ 'rankdir': 'LR'})\n",
        "        # dot.node(self.start.label,self.start.label,shape=\"circle\")\n",
        "        # dot.node(self.accept.label,self.accept.label,shape=\"doublecircle\")\n",
        "        for s in self.inner_states:\n",
        "            if s.label==self.accept.label:\n",
        "                dot.node(s.label,s.label,shape=\"doublecircle\")\n",
        "            else:\n",
        "                dot.node(s.label,s.label,shape=\"circle\")\n",
        "            for e in s.Outedges:\n",
        "                dot.edge(e.src.label,e.dist.label,e.label)\n",
        "        dot.render('test-output/NFA.gv', view=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "GImioDh4rnlO"
      },
      "outputs": [],
      "source": [
        "def Concat(stack:list[NFA]):\n",
        "    if (len(stack) < 2):\n",
        "        print(\"No enough elements\")\n",
        "        return stack\n",
        "\n",
        "    nfa2:NFA = stack.pop()\n",
        "    nfa1:NFA = stack.pop()\n",
        "\n",
        "    newEdges = edge(nfa1.accept,nfa2.start)\n",
        "    nfa1.accept.add_edge(newEdges)\n",
        "\n",
        "    resNfa = NFA(nfa1.start,nfa2.accept,nfa1.inner_states+nfa2.inner_states)\n",
        "    resNfa.allStates.update(nfa1.allStates)\n",
        "    resNfa.allStates.update(nfa2.allStates)\n",
        "    stack.append(resNfa)\n",
        "\n",
        "    return stack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "ZbY0WEIArnlO"
      },
      "outputs": [],
      "source": [
        "def Or(stack:list[NFA]):\n",
        "    if (len(stack) < 2):\n",
        "        print(\"No enough elements\")\n",
        "        return stack\n",
        "\n",
        "    nfa1 = stack.pop()\n",
        "    nfa2 = stack.pop()\n",
        "\n",
        "    newStart = state([])\n",
        "    # sid+=1\n",
        "    newEnd = state([])\n",
        "    # sid+=1\n",
        "\n",
        "    newEdges1 = edge(newStart,nfa1.start)\n",
        "    newEdges2 = edge(newStart,nfa2.start)\n",
        "\n",
        "    newStart.add_edge(newEdges1)\n",
        "    newStart.add_edge(newEdges2)\n",
        "\n",
        "    newEdges3 = edge(nfa1.accept,newEnd)\n",
        "    newEdges4 = edge(nfa2.accept,newEnd)\n",
        "\n",
        "    nfa1.accept.add_edge(newEdges3)\n",
        "    nfa2.accept.add_edge(newEdges4)\n",
        "\n",
        "    resNfa = NFA(newStart,newEnd,nfa1.inner_states+nfa2.inner_states+[newStart,newEnd])\n",
        "    resNfa.allStates.update(nfa1.allStates)\n",
        "    resNfa.allStates.update(nfa2.allStates)\n",
        "    stack.append(resNfa)\n",
        "\n",
        "    return stack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "skiw7YuyrnlO"
      },
      "outputs": [],
      "source": [
        "def ZeroMore(stack:list[NFA]):\n",
        "    if (len(stack) < 1):\n",
        "        print(\"No enough elements\")\n",
        "        return stack\n",
        "    nfa = stack.pop()\n",
        "\n",
        "    newEdges1 = edge(nfa.accept,nfa.start)\n",
        "    nfa.accept.add_edge(newEdges1)\n",
        "\n",
        "    newStart = state([])\n",
        "    # sid+=1\n",
        "    newEnd = state([])\n",
        "    # sid+=1\n",
        "\n",
        "    newEdges2 = edge(newStart,nfa.start)\n",
        "    newStart.add_edge(newEdges2)\n",
        "\n",
        "    newEdges3 = edge(nfa.accept,newEnd)\n",
        "    nfa.accept.add_edge(newEdges3)\n",
        "\n",
        "    newEdges4 = edge(newStart,newEnd)\n",
        "    newStart.add_edge(newEdges4)\n",
        "\n",
        "    resNfa = NFA(newStart,newEnd,nfa.inner_states+[newStart,newEnd])\n",
        "    resNfa.allStates.update(nfa.allStates)\n",
        "\n",
        "    stack.append(resNfa)\n",
        "\n",
        "    return stack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "VBtnwZFXrnlP"
      },
      "outputs": [],
      "source": [
        "def OneMore(stack:list[NFA]):\n",
        "    if (len(stack) < 1):\n",
        "        print(\"No enough elements\")\n",
        "        return stack\n",
        "\n",
        "    nfa = stack.pop()\n",
        "\n",
        "    newEdges1 = edge(nfa.accept,nfa.start)\n",
        "    nfa.accept.add_edge(newEdges1)\n",
        "\n",
        "    newStart = state([])\n",
        "    # sid+=1\n",
        "    newEnd = state([])\n",
        "    # sid+=1\n",
        "\n",
        "    newEdges2 = edge(newStart,nfa.start)\n",
        "    newStart.add_edge(newEdges2)\n",
        "\n",
        "    newEdges3 = edge(nfa.accept,newEnd)\n",
        "    nfa.accept.add_edge(newEdges3)\n",
        "\n",
        "    resNfa = NFA(newStart,newEnd,nfa.inner_states+[newStart,newEnd])\n",
        "    resNfa.allStates.update(nfa.allStates)\n",
        "    stack.append(resNfa)\n",
        "\n",
        "    return stack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "jvabDx19rnlP"
      },
      "outputs": [],
      "source": [
        "def ZeroOne(stack:list[NFA]):\n",
        "    if (len(stack) < 1):\n",
        "        print(\"No enough elements\")\n",
        "        return stack\n",
        "\n",
        "    nfa = stack.pop()\n",
        "\n",
        "    newStart = state([])\n",
        "    # sid+=1\n",
        "    newEnd = state([])\n",
        "    # sid+=1\n",
        "\n",
        "    newEdges1 = edge(newStart,nfa.start)\n",
        "    newStart.add_edge(newEdges1)\n",
        "\n",
        "    newEdges2 = edge(nfa.accept,newEnd)\n",
        "    nfa.accept.add_edge(newEdges2)\n",
        "\n",
        "    newEdges3 = edge(newStart,newEnd)\n",
        "    newStart.add_edge(newEdges3)\n",
        "\n",
        "    resNfa = NFA(newStart,newEnd,nfa.inner_states+[newStart,newEnd])\n",
        "    resNfa.allStates.update(nfa.allStates)\n",
        "    stack.append(resNfa)\n",
        "\n",
        "    return stack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "1Uvy-BuYrnlQ"
      },
      "outputs": [],
      "source": [
        "def Construct(stack:list[NFA],s:str):\n",
        "\n",
        "    start=state([])\n",
        "    # sid+=1\n",
        "    accept=state([])\n",
        "    # sid+=1\n",
        "\n",
        "    newEdges = edge(start,accept,s)\n",
        "    start.add_edge(newEdges)\n",
        "\n",
        "    resNfa = NFA(start,accept,[start,accept])\n",
        "    resNfa.allStates.update({start.label:start,accept.label:accept})\n",
        "    stack.append(resNfa)\n",
        "\n",
        "    return stack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "ug4QbD2DrnlQ"
      },
      "outputs": [],
      "source": [
        "all_states = {}\n",
        "def PostfixToNFA(postfix:str):\n",
        "    alphanum = {char:char for char in \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789.\"}\n",
        "    stack = []\n",
        "    for c in postfix:\n",
        "        if c in alphanum:\n",
        "            stack = Construct(stack,c)\n",
        "        elif c == \"*\":\n",
        "            stack = ZeroMore(stack)\n",
        "        elif c == \"+\":\n",
        "            stack = OneMore(stack)\n",
        "        elif c == \"?\":\n",
        "            stack = ZeroOne(stack)\n",
        "        elif c == \"&\":\n",
        "            stack = Concat(stack)\n",
        "        elif c == \"|\":\n",
        "            stack = Or(stack)\n",
        "        else:\n",
        "            print(\"Invalid postfix expression character {c} is not recognized\")\n",
        "            return None\n",
        "    if len(stack)!=1:\n",
        "        print(\"Invalid postfix expression\")\n",
        "        return None\n",
        "\n",
        "    result = stack.pop()\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gu155dNArnlQ",
        "outputId": "e3e43c2c-8e15-46c8-f0e7-965fa593f0ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "calling validation\n",
            "Passed all test cases ðŸ¥‚ðŸ¥³\n",
            "ab&c&d|\n",
            "<class '__main__.NFA'>\n"
          ]
        }
      ],
      "source": [
        "# test cell\n",
        "# expression = \"(A+B*)?(C|D)\"\n",
        "# postfix = \"A+B*.?CD|.\"\n",
        "# regex = '[0-2]'\n",
        "regex=\"abc|d\"\n",
        "postfix= fromRegToPostfix(regex)\n",
        "print(postfix)\n",
        "if postfix == 'Invalid Regex':\n",
        "    print('Invalid Regex')\n",
        "    exit(-1)\n",
        "nfa = PostfixToNFA(postfix)\n",
        "nfa.start.isStart = True\n",
        "nfa.accept.isTerminating = True\n",
        "print(type(nfa))\n",
        "nfa.display()\n",
        "nfa.graph()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fw9zaPnvrnlQ"
      },
      "source": [
        "## Here we should apply NFA to DFA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9sIFfNQrnlR"
      },
      "source": [
        "### Subset construction algorithm:\n",
        "1. Create the initial state of the DFA by taking the closure of the NFA's initial state.\n",
        "2. For each state in the DFA, and for each possible input symbol:\n",
        "    1. Apply the NFA's transition function to the states in the DFA to determine the next state.\n",
        "    2. Take the closure of the set of states obtained in the previous step.\n",
        "    3. This set of states is the next state of the DFA.\n",
        "    4. Add this transition to the DFA.\n",
        "    5. If the next state is not already in the DFA, add it to the DFA.\n",
        "    6. Repeat steps 2-5 until all states in the DFA have been processed.\n",
        "    7. The final states of the DFA are those that contain a final state of the NFA.\n",
        "    8. The initial state of the DFA is the set of states containing the initial state of the NFA.\n",
        "    9. The alphabet of the DFA is the same as the alphabet of the NFA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "XzdgJUvGrnlR"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "    Create a class called dumbState, its attribute are\n",
        "        1. state: the state itself\n",
        "        2. isTerminating: a boolean to indicate if the state is terminating or not.\n",
        "        3. transition:\n",
        "            it should be a map\n",
        "                its key is the label\n",
        "                its value is the destination state.\n",
        "'''\n",
        "class dumbState:\n",
        "    def __init__(self, state, isTerminating, transitions = {}, isStarting = False):\n",
        "        self.state = state\n",
        "        self.isStarting = isStarting\n",
        "        self.isTerminating = isTerminating\n",
        "        self.transitions = transitions\n",
        "\n",
        "    def addTransition(self, label, destination):\n",
        "        self.transitions[label] = destination\n",
        "\n",
        "'''\n",
        "    Create a class of superNode, which hold a list of dumbStates, and wethere it is a terminating state or not.\n",
        "    Also it should carry a map of transitions, its key is the alphanumeric, and the value is a new superNode.\n",
        "    the label is like S1 S2 ...\n",
        "'''\n",
        "class superNode:\n",
        "    def __init__(self, states, isTerminating,transitions = [],label=\"\",isStarting = False ):\n",
        "        self.states = states\n",
        "        self.isStarting = isStarting\n",
        "        self.isTerminating = isTerminating\n",
        "        self.transitions = transitions\n",
        "        self.label=label\n",
        "    def addState(self, state):\n",
        "        self.states.append(state)\n",
        "    def markAsTerminating(self, mark):\n",
        "        self.isTerminating = mark\n",
        "    def markAsStarting(self, mark):\n",
        "        self.isStarting = mark\n",
        "\n",
        "    def addTransition(self, label, destination):\n",
        "        self.transitions.append({label: destination})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "7RYK9f4jrnlS"
      },
      "outputs": [],
      "source": [
        "# Hosny code\n",
        "class DFA :\n",
        "    def __init__(self, start, accept, transitions):\n",
        "        self.start:superNode = start\n",
        "        self.accept:list[state] = accept\n",
        "        self.transitions = transitions\n",
        "\n",
        "    def display(self):\n",
        "        with open('dfa.json', 'w') as f:\n",
        "            f.write(\"{\\n\")\n",
        "            f.write(\"\\\"startingState\\\":\\\"\"+self.start.label+\"\\\",\\n\")\n",
        "            for s in self.transitions:\n",
        "                f.write(\"\\\"\"+s.label+\"\\\":\\n\")\n",
        "                f.write(\"   {\\n\")\n",
        "                f.write(\"       \\\"isTerminatingState\\\":\\\"\"+str(s in self.accept)+\"\\\"\")\n",
        "                if len(s.transitions)>0:\n",
        "                    f.write(\",\\n\")\n",
        "                for e in s.transitions:\n",
        "                    ##! To be changed\n",
        "                    if (type(e) != dict):\n",
        "                        continue\n",
        "                    label=list(e.keys())[0]\n",
        "                    dist= list(e.values())[0]\n",
        "                    f.write(\"       \\\"\"+label+\"\\\":\\\"\"+dist.label+\"\\\"\")\n",
        "                    if e!=list(s.transitions)[-1]:\n",
        "                        f.write(\",\\n\")\n",
        "                    else:\n",
        "                        f.write(\"\\n\")\n",
        "                f.write(\"   }\")\n",
        "                if s!=self.transitions[-1]:\n",
        "                    f.write(\",\\n\")\n",
        "                else:\n",
        "                    f.write(\"\\n\")\n",
        "            f.write(\"}\\n\")\n",
        "\n",
        "    def graph (self):\n",
        "        dot = Digraph(comment='DFA graph',format='png',graph_attr={ 'rankdir': 'LR'})\n",
        "        # dot.node(self.start.label,self.start.label,shape=\"circle\")\n",
        "        # dot.node(self.accept.label,self.accept.label,shape=\"doublecircle\")\n",
        "        acceptedLabels = []\n",
        "        for acceptNode in self.accept:\n",
        "            acceptedLabels.append(acceptNode.label)\n",
        "        for s in self.transitions:\n",
        "            if s.label in acceptedLabels:\n",
        "                dot.node(s.label,s.label,shape=\"doublecircle\")\n",
        "            else:\n",
        "                dot.node(s.label,s.label,shape=\"circle\")\n",
        "            for k in s.transitions:\n",
        "                if (type(k) != dict):\n",
        "                    continue\n",
        "                label=list(k.keys())[0]\n",
        "                k=list(k.values())[0]\n",
        "                dot.edge(s.label,k.label,label)\n",
        "                # dot.edge(k.src.label,v.dist.label,v.label)\n",
        "        dot.render('test-output/DFA.gv', view=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "JUooV8-zrnlS"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "    I want to create a function that correct the directions of the edges.\n",
        "    it will follow the following algorithm:\n",
        "        1. iterate over the list of supernodes.\n",
        "        2. for each transition\n",
        "            2.1. get the label\n",
        "            2.2. iterate over all supernode labels.\n",
        "            2.3. when you find the label inwhich it exist.\n",
        "            2.4. replace the old with the new\n",
        "    #! This function must be called after the DFA_from_NFA function\n",
        "'''\n",
        "# id = 0\n",
        "\n",
        "\n",
        "def correctEdgesDirections (superNodes: [superNode]) :\n",
        "    # global id\n",
        "    for node in superNodes:\n",
        "        for key in node.transitions:\n",
        "            key = list(key.values())[0]\n",
        "            for s in superNodes:\n",
        "                # check that node.transitions[key] is substring of s.label\n",
        "                if  s.label.find(key.label) != -1:\n",
        "                    key.label = s.label  #'s' + str(id)  # here we should make the name shorter.\n",
        "                    break\n",
        "        # s.label = 's' + str(id)\n",
        "        # id += 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "qIkGJJMWrnlT"
      },
      "outputs": [],
      "source": [
        "#global map of all states\n",
        "statesToSuperNode = {}\n",
        "\n",
        "def epsilon_closure(s:state):\n",
        "    closure = set()\n",
        "    closure.add(s)\n",
        "    for e in s.Outedges:\n",
        "        if e.label == \"@\":\n",
        "            closure = closure.union(epsilon_closure(e.dist))\n",
        "    return closure\n",
        "\n",
        "def DFA_from_NFA(nfa:NFA):\n",
        "    global statesToSuperNode\n",
        "    \"\"\"Convert NFA to DFA.\"\"\"\n",
        "\n",
        "    # apply epsilon closure on the start state of the nfa\n",
        "    start_closure = epsilon_closure(nfa.start)\n",
        "    accept = False\n",
        "    newLabel = \"\"\n",
        "\n",
        "    for s in start_closure:\n",
        "        # v\n",
        "        if s.label == nfa.accept.label:\n",
        "            accept = True\n",
        "        newLabel+=s.label\n",
        "        # if s != list(start_closure)[-1]:\n",
        "        #     newLabel+=\",\"\n",
        "    newStart = superNode(start_closure,accept,[],newLabel)\n",
        "\n",
        "    created_states = {}\n",
        "    unvisited_states = []\n",
        "    # get new state label\n",
        "    created_states[newLabel] = newStart\n",
        "\n",
        "\n",
        "    for s in start_closure:\n",
        "        statesToSuperNode[s] = newStart\n",
        "        for e in s.Outedges :\n",
        "            if e.label != \"@\":\n",
        "                newStart.addTransition(e.label,e.dist)\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "    for key in newStart.transitions:\n",
        "        key = list(key.values())[0]\n",
        "        if key in created_states:\n",
        "            continue\n",
        "        else:\n",
        "            states=nfa.allStates[key.label]\n",
        "            newState=superNode([states],False)\n",
        "            newState.label = key.label\n",
        "            created_states[newState.label] = newState\n",
        "            unvisited_states.append(newState)\n",
        "\n",
        "    dfa=DFA(newStart,[nfa.accept],[])\n",
        "\n",
        "    dfa.transitions.append(newStart)\n",
        "\n",
        "\n",
        "    if accept:\n",
        "        dfa.accept.append(newStart)\n",
        "\n",
        "\n",
        "    while len(unvisited_states)>0:\n",
        "        current_state:superNode = unvisited_states.pop(0)\n",
        "        current_state_closure=set()\n",
        "        for s in current_state.states:\n",
        "            if s is None:\n",
        "                continue\n",
        "            current_state_closure = current_state_closure.union(epsilon_closure(s))\n",
        "        newLabel=\"\"\n",
        "        accept = False\n",
        "        for s in current_state_closure:\n",
        "            if s is None:\n",
        "                continue\n",
        "            if s.label == nfa.accept.label:\n",
        "                accept=True\n",
        "            newLabel+=s.label\n",
        "        newState = superNode(current_state_closure,accept,[],newLabel)\n",
        "        created_states[newLabel] = newState\n",
        "        if accept:\n",
        "            dfa.accept.append(newState)\n",
        "        for s in current_state_closure:\n",
        "                \n",
        "            statesToSuperNode[s] = newStart\n",
        "            if s is None:\n",
        "                continue\n",
        "            for e in s.Outedges:\n",
        "                if e.label != \"@\":\n",
        "                    newState.addTransition(e.label,e.dist)\n",
        "\n",
        "        for key in newState.transitions:\n",
        "            key = list(key.values())[0]\n",
        "            if key in created_states:\n",
        "                continue\n",
        "            else:\n",
        "                states=None\n",
        "                try:\n",
        "                    states=nfa.allStates[key.label]\n",
        "                except:\n",
        "                    states=None\n",
        "                new_state=superNode([states],False)\n",
        "                new_state.label = key\n",
        "                created_states[key] = new_state\n",
        "                unvisited_states.append(new_state)\n",
        "        dfa.transitions.append(newState)\n",
        "    return dfa\n",
        "\n",
        "\n",
        "# 01153"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "PbLwu8UArnlT"
      },
      "outputs": [],
      "source": [
        "# Now I need a function, that convert from the generated DFA to minimized DFA\n",
        "\n",
        "# dfa = DFA_from_NFA(nfa)\n",
        "# correctEdgesDirections(dfa.transitions)\n",
        "\n",
        "# dfa.display()\n",
        "# dfa.graph()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIrO5CXtrnlT"
      },
      "source": [
        "### Now DFA minimization\n",
        "* Algorithm:\n",
        "    1. Create a table of all pairs of states\n",
        "    2. Mark all pairs of states that are distinguishable\n",
        "    3. Repeat until no more pairs can be marked:\n",
        "        1. For each pair of states (p, q) that is not marked:\n",
        "            1. For each input symbol a:\n",
        "                1. If the pair (Î´(p, a), Î´(q, a)) is marked, mark (p, q)\n",
        "                2. Otherwise, mark (p, q)\n",
        "                3. If any new pairs were marked, go back to step 3\n",
        "                4. Combine all unmarked pairs into a single state\n",
        "                5. The resulting states are the minimized DFA states\n",
        "\n",
        "\n",
        "                "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwGlZsdKrnlT"
      },
      "source": [
        "### Hand Analysis and algorithm\n",
        "![page1](attachment:image-2.png)\n",
        "![page2](attachment:image.png)\n",
        "![page3](attachment:image-3.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "CTVV4R7IrnlU"
      },
      "outputs": [],
      "source": [
        "def initializeMat(states:[dumbState]):\n",
        "    # build nxn matrix, where n is the number of states\n",
        "    matrix = [['T' for i in range(len(states))] for j in range(len(states))]\n",
        "\n",
        "    # diffrentiate between terminating, and non terminating variables only.\n",
        "    for i in range(len(matrix)):\n",
        "        for j in range(i):\n",
        "            if states[i].isTerminating != states[j].isTerminating: # if one is terminating, and the other is not, then mark as not same\n",
        "                matrix[i][j] = 'F'\n",
        "                matrix[j][i] = 'F'\n",
        "\n",
        "    return matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_yKuwMMrnlU",
        "outputId": "493e1ba7-142e-40ee-f9e6-1a25422de6bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "False\n"
          ]
        }
      ],
      "source": [
        "a = ['a',1,4]\n",
        "b = [4, 1 , 'a']\n",
        "print(a==b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "def convertFromListToMap(listOfTransitions):\n",
        "    result = {}\n",
        "    for transition in listOfTransitions:\n",
        "        key = list(transition.keys())[0]\n",
        "        value = list(transition.values())[0]\n",
        "        result[key] = value\n",
        "    return result\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "GpQ9xaM-rnlU"
      },
      "outputs": [],
      "source": [
        "\n",
        "def markUnsimilarStates(matrix, states:[dumbState], transitions, conversionMap):\n",
        "    noChangesOccured = False\n",
        "    convMapKeys = list(conversionMap.keys())\n",
        "    while (not noChangesOccured):\n",
        "        noChangesOccured = True\n",
        "        for i in range(len(matrix)):\n",
        "            for j in range(i):\n",
        "               if  matrix[i][j] == 'T' :\n",
        "                   iTransitions = convertFromListToMap(states[i].transitions)\n",
        "                   jTransitions = convertFromListToMap(states[j].transitions)\n",
        "                   # check if two lists content are not the same\n",
        "                   if len(iTransitions) != len(jTransitions) or sorted([iTransitions.keys()]) != sorted([jTransitions.keys()]):\n",
        "                          matrix[i][j] = 'F'\n",
        "                          matrix[j][i] = 'F'\n",
        "                          noChangesOccured = False\n",
        "                          continue\n",
        "                   # check if the transitions are not the same\n",
        "                   commonTrans = list(iTransitions.keys())\n",
        "                   commonTrans += list(jTransitions.keys())\n",
        "                   commonTrans = list(set(commonTrans))\n",
        "\n",
        "                   for trans in commonTrans:\n",
        "                   \n",
        "                       s1T = iTransitions[trans].label\n",
        "                       s2T = jTransitions[trans].label\n",
        "                       row = conversionMap.get(s1T, None)\n",
        "                       col = conversionMap.get(s2T, None)\n",
        "                       if row is None or col is None:\n",
        "                           continue\n",
        "                    #    if (s1T in convMapKeys and s2T in convMapKeys) and  matrix[conversionMap[s1T]][conversionMap[s2T]] == 'F' or matrix[conversionMap[s2T]][conversionMap[s1T]] == 'F': # because order does not matter, so double check on both.\n",
        "                       if matrix[row][col] == 'F' or matrix[row][col] == 'F': # because order does not matter, so double check on both.\n",
        "                           matrix[i][j] = 'F'\n",
        "                           matrix[j][i] = 'F'\n",
        "                           noChangesOccured = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "ciwrC7IvrnlV"
      },
      "outputs": [],
      "source": [
        "def combineSimilarNodes(matrix, states:[dumbState]):\n",
        "    # create a list of supernodes\n",
        "    superNodes = [ ]\n",
        "\n",
        "    # create a bool matrix, to indicate whether we have inserted this node before in a supernode or not\n",
        "    inserted = [False for i in range(len(states))]\n",
        "    for i in range(len(states)):\n",
        "        if not inserted[i]:\n",
        "            supernode = superNode([states[i]], states[i].isTerminating, [], states[i].state, states[i].isStarting)\n",
        "            for j in range(i+1, len(states)):\n",
        "                if matrix[i][j] == 'T':\n",
        "                    supernode.addState(states[j])\n",
        "                    if states[j].isTerminating:\n",
        "                        supernode.markAsTerminating(True)\n",
        "                    if states[j].isStarting:\n",
        "                        supernode.markAsStarting(True)\n",
        "                    inserted[j] = True\n",
        "            superNodes.append(supernode)\n",
        "\n",
        "    return superNodes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "a_NmcJgrrnlV"
      },
      "outputs": [],
      "source": [
        "def checkIfTheNodeExistsBefore(minimizedDFAList:list[dumbState], sn:dumbState):\n",
        "    # Extract the first state\n",
        "    firstState = sn\n",
        "    existBefore = False\n",
        "    newNode = None\n",
        "\n",
        "    # Search for the node in the created nodes\n",
        "    for node in minimizedDFAList:\n",
        "        if firstState.state == node.state: # exist in the list\n",
        "            existBefore = True\n",
        "            newNode = node\n",
        "            break\n",
        "\n",
        "    if not existBefore:\n",
        "        newNode = dumbState(firstState.state, sn.isTerminating, {})\n",
        "    return newNode\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "dsk04NFkrnlW"
      },
      "outputs": [],
      "source": [
        "def lookForNode(dest:state, superNodeStates:list[dumbState]):\n",
        "    # extract all the labels of superNodes\n",
        "    labels = []\n",
        "    for node in superNodeStates:\n",
        "        labels.append(node.state)\n",
        "    # search by labels\n",
        "    return dest.label in labels\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "zX1oHwFUrnlW"
      },
      "outputs": [],
      "source": [
        "def minimizingSuperNodes(superNodes:list[superNode]):\n",
        "    '''\n",
        "        1. create a list of super nodes which represent the minimized DFA\n",
        "        2. Iterate over the given superNodes\n",
        "            1. extract the first state\n",
        "            2. check if the label of this state exist in the new list\n",
        "            3. if not create new supernode and initialize it with the properties of the first state\n",
        "            4. iterate over all transitions\n",
        "                1. extract the label of the destination\n",
        "                2. iterate over the list of superNodes\n",
        "                    1. search for that label in each state of the superNode\n",
        "                    2. extract the label of the first state of the found superNode\n",
        "                    3. if exist in the new supernodes list, asign it as the new destination\n",
        "                    4. else, create new super node with its name, and append it to the list of superNodes\n",
        "\n",
        "            4. append the new created supernode to the list of superNodes\n",
        "\n",
        "    '''\n",
        "    minimizedDFAList:list[dumbState] = []\n",
        "    for sn in superNodes:\n",
        "        # Extract the first state\n",
        "        firstState = sn.states[0]\n",
        "\n",
        "        # creating or extracting a supernode\n",
        "        extractedNode = checkIfTheNodeExistsBefore(minimizedDFAList, firstState)\n",
        "\n",
        "        # getting the transitions of the representative state in the super node.\n",
        "        snTransitions =  sn.states[0].transitions\n",
        "\n",
        "        # iterate over its transitions.\n",
        "        for labelAndTransition in snTransitions:\n",
        "            label = list(labelAndTransition.keys())[0]\n",
        "            destDumbNode =  list(labelAndTransition.values())[0]\n",
        "            # print(label, destDumbNode.label)\n",
        "\n",
        "            # Extracting the destination\n",
        "            # destDumbNode = snTransitions[labelAndTransition] # B or C -> this is state not dumbState\n",
        "            # iterate over the given supernodes\n",
        "            for superNode in superNodes:\n",
        "              # look for the destination in each super node\n",
        "\n",
        "              if lookForNode(destDumbNode, superNode.states):\n",
        "                # extract the representer of target supernode\n",
        "                repDumbNode = superNode.states[0]\n",
        "\n",
        "                # check if it was created before, if yes, return the original, else return new state\n",
        "                newDestination = checkIfTheNodeExistsBefore(minimizedDFAList, repDumbNode)\n",
        "                # add transition to the extracted node\n",
        "                extractedNode.addTransition(label, newDestination)\n",
        "\n",
        "        # Check that the extracted node was not inserted before\n",
        "        if extractedNode not in minimizedDFAList:\n",
        "\n",
        "            # insert new minimized Node\n",
        "            minimizedDFAList.append(extractedNode)\n",
        "    return minimizedDFAList\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "def printMinDFA(minimizedDFA): \n",
        "    # printing the minimized DFA\n",
        "    print('Minimized DFA')\n",
        "    for state in minimizedDFA:\n",
        "        print(state.state)\n",
        "        for item in state.transitions:\n",
        "            print(item, state.transitions[item].state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "1djP4I1FrnlX"
      },
      "outputs": [],
      "source": [
        "def minimizingDFA (states: list[dumbState], transitions):\n",
        "    #? Checked and Correct\n",
        "    conversionMap = {\n",
        "         states[i].state:i for i in range(len(states))\n",
        "    }\n",
        "    #? Checked and Correct\n",
        "    # initialize the matrix.\n",
        "    matrix = initializeMat(states)\n",
        "\n",
        "    #! what should I do if two states have different transitions? should I just say that they are not the same and truncate?\n",
        "    # mark unsimilar states\n",
        "    markUnsimilarStates(matrix, states, transitions, conversionMap)\n",
        "\n",
        "    # comibne similar nodes into superNodes\n",
        "    superNodes = combineSimilarNodes(matrix, states)\n",
        "\n",
        "\n",
        "    # now we need to connect each super node with the other super node\n",
        "    minimizedDFA = minimizingSuperNodes(superNodes)\n",
        "    if(not minimizedDFA):\n",
        "        return\n",
        "    # printMinDFA(minimizedDFA)\n",
        "\n",
        "    return minimizedDFA\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "X4lSoASOrnlX"
      },
      "outputs": [],
      "source": [
        "def prepareDFA(dfa:DFA):\n",
        "\n",
        "    # 1. create a list of states, and transitions\n",
        "    states = []\n",
        "    allTransitions = []\n",
        "\n",
        "    # 2. iterate over all nodes in the dfa\n",
        "    for node in dfa.transitions:\n",
        "        # 3. extract all needed information\n",
        "        isTerminating = node.isTerminating\n",
        "        transitions = node.transitions\n",
        "        isStart = node == dfa.start\n",
        "        for key in transitions:\n",
        "            allTransitions.append(key)\n",
        "        # 4. create new dumbState, and append it to the list of states\n",
        "        states.append(dumbState(node.label,isTerminating,transitions, isStart))\n",
        "    return states, allTransitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "def DisplayMinimizedDFA(minimizedDFA:list[dumbState]):\n",
        "    startingNodeLabel = ''\n",
        "    for node in minimizedDFA: \n",
        "        if node.isStarting:\n",
        "            startingNodeLabel = node.state\n",
        "            break\n",
        "    \n",
        "\n",
        "    with open('minimizedDFA.json', 'w') as f:\n",
        "        f.write(\"{\\n\")\n",
        "        # assume the first state is the start state\n",
        "        f.write(\"\\\"startingState\\\":\\\"\"+startingNodeLabel+\"\\\",\\n\")\n",
        "\n",
        "        for s in minimizedDFA:\n",
        "            f.write(\"\\\"\"+s.state+\"\\\":\\n\")\n",
        "            f.write(\"   {\\n\")\n",
        "            f.write(\"       \\\"isTerminatingState\\\":\\\"\"+str(s.isTerminating)+\"\\\"\")\n",
        "            if len(s.transitions)>0:\n",
        "                f.write(\",\\n\")\n",
        "            for e in s.transitions:\n",
        "                f.write(\"       \\\"\"+e+\"\\\":\\\"\"+s.transitions[e].state+\"\\\"\")\n",
        "                if e!=list(s.transitions)[-1]:\n",
        "                    f.write(\",\\n\")\n",
        "                else:\n",
        "                    f.write(\"\\n\")\n",
        "            f.write(\"   }\")\n",
        "            if s!=minimizedDFA[-1]:\n",
        "                f.write(\",\\n\")\n",
        "            else:\n",
        "                f.write(\"\\n\")\n",
        "        f.write(\"}\\n\")\n",
        "\n",
        "def DisplayMinimizedDFAGraph(minimizedDFA:list[dumbState]):\n",
        "    dot = Digraph(comment='Minimized DFA graph',format='png',graph_attr={ 'rankdir': 'LR'})\n",
        "    acceptedLabels = []\n",
        "    for acceptNode in minimizedDFA:\n",
        "        if acceptNode.isTerminating:\n",
        "            acceptedLabels.append(acceptNode.state)\n",
        "    for s in minimizedDFA:\n",
        "        if s.state in acceptedLabels:\n",
        "            dot.node(s.state,s.state,shape=\"doublecircle\")\n",
        "        else:\n",
        "            dot.node(s.state,s.state,shape=\"circle\")\n",
        "        for k in s.transitions:\n",
        "            dot.edge(s.state,s.transitions[k].state,k)\n",
        "    dot.render('test-output/minimizedDFA.gv', view=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# class state:\n",
        "#     def __init__(self,edges, isTerminating = False, isStart= False):\n",
        "#         global sid\n",
        "#         self.Outedges:list[edge] = edges\n",
        "#         self.label = \"S\"+str(sid)\n",
        "#         self.isTerminating = isTerminating\n",
        "#         self.isStart = isStart\n",
        "#         sid+=1\n",
        "#     def add_edge(self,edge):\n",
        "#         self.Outedges.append(edge)\n",
        "\n",
        "#         # assume @ is the epsilon symbol\n",
        "# class edge:\n",
        "#     def __init__(self,src,dist,label=\"@\"):\n",
        "#         self.src = src\n",
        "#         self.dist = dist\n",
        "#         self.label = label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def getNoneAtDest(s:state): \n",
        "#     # iterate till the edge is not @\n",
        "#     pass\n",
        "#     # if e.label != '@':\n",
        "#     #     return e.dist\n",
        "#     # return getNoneAtDest(e.dist)\n",
        "\n",
        "def getNoneAtDest(s:state):\n",
        "    closure = set()\n",
        "    closure.add(s)\n",
        "    for e in s.Outedges:\n",
        "        if e.label == \"@\":\n",
        "            closure = closure.union(epsilon_closure(e.dist))\n",
        "    return closure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extractAllTransitionsFromEdges(edges:list[edge]):\n",
        "    # for each edge => src, dest, label\n",
        "    # extract the label and the destination, and add them in a map, and append them to the list \n",
        "    transitionsList = [] \n",
        "    for e in edges: \n",
        "        # if e.label != '@':\n",
        "        destnations = getNoneAtDest(e.dist)\n",
        "        transitionsList.append({e.label: destnations})\n",
        "    return transitionsList"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "def createDumbNodeFromState(s:state):\n",
        "    # create a dumbNode from the state\n",
        "    # extract the label, isTerminating, isStarting, and the transitions\n",
        "    transMap = {}\n",
        "    for e in s.Outedges: \n",
        "        transMap[e.label] = e.dist \n",
        "    # print(transMap)\n",
        "    return dumbState(s.label, s.isTerminating, transMap, s.isStart)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "def addNeighbours (node, transitions): \n",
        "    neighbours = getNoneAtDest(node)\n",
        "    for nei in neighbours: \n",
        "        for e in nei.Outedges: \n",
        "            if e.label != '@' and e.dist not in neighbours:\n",
        "                transitions.append({e.label:e.dist})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mergeNodes(listOfStates:list[state], currentNode:superNode, transition: str, dfa:DFA):\n",
        "    # create new supernode \n",
        "    isTerminate = False\n",
        "    isStart = False\n",
        "    transitions = []\n",
        "    states = []\n",
        "    label = ''\n",
        "    for node in listOfStates:\n",
        "        print(type(node))\n",
        "        if type(node) is superNode:\n",
        "            continue\n",
        "        # Debug pos\n",
        "        isTerminate |= node.isTerminating\n",
        "        # isStart |= node.isStart\n",
        "        addNeighbours(node, transitions)\n",
        "        # transitions += extractAllTransitionsFromEdges(node.Outedges)\n",
        "        dumbNode = createDumbNodeFromState(node)\n",
        "        states.append(dumbNode)\n",
        "        label += node.label\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "    newSuperNode = superNode(states, isTerminate, transitions, label, False)\n",
        "\n",
        "    \n",
        "\n",
        "    # now merge the current node with the new super node\n",
        "    currentNode.addTransition(transition, newSuperNode) \n",
        "\n",
        "    # add the transition in the dfa \n",
        "    dfa.transitions.append(newSuperNode)   \n",
        "    if newSuperNode.isTerminating : \n",
        "        dfa.accept.append(newSuperNode)\n",
        "\n",
        "    return newSuperNode \n",
        "    \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mergeDFANodes (dfa): \n",
        "    \n",
        "    genNodesLabels = []\n",
        "    # iterate over each node in the dfa \n",
        "    for sn in dfa.transitions: \n",
        "        \n",
        "        # print(type(sn))\n",
        "        # get all the transitions \n",
        "        listOfMaps = sn.transitions\n",
        "        '''\n",
        "            create a map \n",
        "                key -> trans: \n",
        "                value -> list of nodes\n",
        "        '''\n",
        "        transitionMap = {}\n",
        "        nodesLabels = []\n",
        "        # iterate over the transition lise\n",
        "        for item in listOfMaps: \n",
        "            if (type(item) != dict):\n",
        "                continue\n",
        "            transChar = list(item.keys())[0]\n",
        "            Node = list(item.values())[0] \n",
        "            if transChar in transitionMap: \n",
        "                transitionMap[transChar].append(Node)\n",
        "                nodesLabels.append(Node.label)\n",
        "            else: \n",
        "                transitionMap[transChar] = [Node]\n",
        "                nodesLabels = [Node.label]\n",
        "\n",
        "        print (nodesLabels)\n",
        "        # now lets merge each item in the transition map \n",
        "        # print(type(transitionMap))\n",
        "        createdNode = None\n",
        "        for char,nodesList in transitionMap.items(): \n",
        "        #    print(nodesList)\n",
        "           if len(nodesList) > 1: \n",
        "               createdNode = mergeNodes(nodesList, sn, char,dfa)\n",
        "               # copy all elements in the nodes LAbel into gen Label \n",
        "               \n",
        "               i = -1\n",
        "               while i < len (dfa.transitions):\n",
        "                    print(i,  len (dfa.transitions))\n",
        "                    i+=1\n",
        "                    if i >= len(dfa.transitions):\n",
        "                        break\n",
        "                    sn = dfa.transitions[i]\n",
        "                    print(\"the current super node is\" +  sn.label)\n",
        "                    if sn.label in nodesLabels:\n",
        "                        print(\"removing: \" + sn.label)\n",
        "                        dfa.transitions.remove(sn)\n",
        "                        i -= 1\n",
        "                    if i < 0:\n",
        "                        break\n",
        "                    \n",
        "                        # continue\n",
        "               genNodesLabels += nodesLabels\n",
        "        print(genNodesLabels)\n",
        "           \n",
        "\n",
        "            \n",
        "            \n",
        "        \n",
        "        # now for each entry in the transitionMap, we need to delete the edges and nodes from the dfa.transitions\n",
        "        # and add the new supernode to the dfa.transitions\n",
        "        # print(transitionMap)\n",
        "        print('-------------------')\n",
        "\n",
        "                        \n",
        "                    \n",
        "        \n",
        "        # now we need to delete previous nodes \n",
        "        \n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "pUwAVCN6rnlY"
      },
      "outputs": [],
      "source": [
        "# This should be the cell in which we add the whole process\n",
        "def main_function(regex:str):\n",
        "    # here we apply validation, lexing, and parsing => frontend part\n",
        "    postfix= fromRegToPostfix(regex)\n",
        "\n",
        "    # print(postfix)\n",
        "    if postfix == 'Invalid Regex':\n",
        "        print('Invalid Regex')\n",
        "        return\n",
        "\n",
        "    # generating nfa from postfix\n",
        "    nfa = PostfixToNFA(postfix)\n",
        "    if (nfa == None):\n",
        "        return\n",
        "    nfa.start.isStart = True\n",
        "    nfa.accept.isTerminating = True\n",
        "    # nfa.display()\n",
        "    # nfa.graph()\n",
        "\n",
        "    # generating dfa from nfa\n",
        "    dfa = DFA_from_NFA(nfa)\n",
        "\n",
        "\n",
        "    # fixing edges\n",
        "    correctEdgesDirections(dfa.transitions)\n",
        "    # dfa.display()\n",
        "    # dfa.graph()\n",
        "    # mergeDFANodes(dfa)\n",
        "    # dfa.display()\n",
        "    # dfa.graph()\n",
        "\n",
        "    # preparing dfa to be minimized\n",
        "    states, transitions = prepareDFA(dfa)\n",
        "\n",
        "    # minimizing DFA\n",
        "    minDfa = minimizingDFA(states, transitions)\n",
        "\n",
        "    # creating the json file\n",
        "    DisplayMinimizedDFA(minDfa)\n",
        "\n",
        "    # printing the graph\n",
        "    DisplayMinimizedDFAGraph(minDfa)\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "PM_tr3CZrnlY"
      },
      "outputs": [],
      "source": [
        "# testing\n",
        "# main_function('ab*')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbZlT493rnlZ"
      },
      "source": [
        "# Now we just need to draw the minimized DFA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrHfpyXarnlZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "c9xmCQZdrnla",
        "outputId": "64a435df-1b3c-4420-cb3c-35b88d45aec7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing: a+b*a\n",
            "calling validation\n",
            "Passed all test cases ðŸ¥‚ðŸ¥³\n",
            "-----------------------------\n"
          ]
        }
      ],
      "source": [
        "# lw feh *, byrsm sahmen\n",
        "testcases = [\n",
        "    # 'a+b*',   # -> fully correct\n",
        "    # 'a+b',   # -> fully correct\n",
        "    # 'a*b*ca',  # -> fully correct\n",
        "    # 'a+|b+',  # -> fully correct\n",
        "    # '[a-z0-9]32', # -> fully correct\n",
        "    # '[a-f0-9]32', # -> fully correct\n",
        "    # 'a*|b*', # -> fully correct\n",
        "    # '[a-c]*', # fully correct\n",
        "    # '[abc](d|e|f)', # fully correct\n",
        "    # '[a-fA-C]', # fully correct\n",
        "    # 'hus|(s(n|n)y)',  # fully correct\n",
        "    # 'zizo|(z(u|o)z)',  # fully correct\n",
        "    # 'a+b+a',  # -> fully correct\n",
        "    # '(a|b)a[ab]', # correct\n",
        "    # '(a*b)(b?a+)'  # -> fully correct\n",
        "    # ---------------------------------------------------\n",
        "    'a+b*a',  # -> bug -> in dfa\n",
        "    # '(a|b)*a[ab]?', # bug\n",
        "    # '(a|b)*a[ab]', # bug\n",
        "    # 'a*b+[a-d](c?)', # inf loop abb\n",
        "    # ==================================================\n",
        "    # '(a*)*' # bt3ml crash lel kernal  # -> this should be invalid case, due to infinte loop\n",
        "    # '[bc]*(cd)+', # -> inf loop\n",
        "    # '(a*b*)([c-d]*)' # inf loop\n",
        "    # '(a+a+)+b',  # -> bug\n",
        "    # '(a|b)+a[ab]?', # bug\n",
        "]\n",
        "\n",
        "for case in testcases:\n",
        "    print('Testing: ' + case)\n",
        "    main_function(case) \n",
        "    # input('Press Enter to continue...')\n",
        "    print('-----------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pkXtVfm2rnla"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "sH0IfLqhrnk_",
        "0kP4GcpqrnlF",
        "hD1rsOxqrnlI"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
